[{"title":"MQ引入","url":"/2025/07/26/MQ%E5%BC%95%E5%85%A5/","content":"同步&#x2F;异步通讯首先，我们知道MQ（Message Queue，消息队列）是一种通信机制，那通讯机制又是什么呢？\n\n通讯机制又分为同步通讯和异步通讯\n同步通讯可以看作是用手机给别人打电话，双方的交互是实时的，这时可以立即得到响应，但是你却不能跟多个人同时通话。\n异步通讯就好比微信发送消息，双方的交互并不是实时的，你可以立即回复消息，也可以等一段时间回复，这样就会使得消息之间的传递有延迟，但是你还可以同时与多个人收发消息。\n\n\n\n所以，如果我们的业务需要实时得到服务提供方的响应，则应该选择同步通讯（同步调用），而如果我们追求更高的效率，并且不需要实时响应，则应该选择异步通讯（异步调用）。\n接下来我们再来理解一下同步调用和异步调用。\n同步调用以下根据一个支付服务对同步业务进行解析\n\n\n首先解释一下业务流程：\n\n支付服务需要先调用用户服务完成余额扣减\n然后支付服务自己要更新支付流水单的状态\n然后支付服务调用交易服务，更新业务订单状态为已支付\n\n三个步骤依次执行。\n看似没有问题，逻辑清晰，但是其中存在三个问题\n\n拓展性差\n\n如果在业务后期需要给它加入一个短信通知业务，积分业务等等。。\n\n\n每次添加业务都需要修改大量代码（不仅要在支付服务的接口中定义新的接口，新的接口又与旧的接口中有交互），非常臃肿，不符合开闭原则，拓展性不好。\n\n性能差\n\n调用者需要等待上一个服务执行完之后，有结果后，才能继续向下执行，也就是说每次调用，调用者都是处于阻塞等待状态。最终整个业务的响应时长就是每次调用的执行时长之和：\n\n\n假如每个微服务的执行时长都是50ms，则最终整个业务的耗时可能高达300ms，性能太差了，用户都等炸毛了。\n\n级联失败\n\n\n“级联失败”（Cascading Failure）是一个在分布式系统中常见的现象，指的是一个系统中的某个组件或服务发生故障时，导致其他相关联的组件或服务也相继发生故障，从而引发整个系统的崩溃或性能大幅下降。\n\n由于我们是基于OpenFeign调用交易服务、通知服务。当交易服务、通知服务出现故障时，整个事务都会回滚，交易失败。\n如果只是因为短信通知故障，导致之前收到的钱又返还了，是很得不偿失的，因为你不知道这个用户是否还会在你这里进行购物。。。。\n（OpenFeign 是一个用于简化 HTTP 请求的 Java 库，主要用于服务间的通信。它通过声明式的方式，使得开发者能够更简洁地发起 RESTful 请求，而不需要显式地编写底层的 HTTP 客户端代码。在这里不用过多了解）\n这其实就是同步调用的级联失败问题。\n针对于这三个问题，我们就必须用异步调用的方式来代替同步调用。\n异步调用异步调用方式其实就是基于消息通知的方式，一般包含三个角色：\n\n消息发送者：投递消息的人，就是原来的调用方（对应上面的例子就是支付服务）\n消息Broker：管理、暂存、转发消息，你可以把它理解成微信服务器。\n消息接收者：接收和处理消息的人，就是原来的服务提供方（对应上面的交易服务，通知服务等等。。）\n\n在异步调用中，发送者不再直接同步调用接收者的业务接口，而是发送一条消息投递给消息Broker。\n然后接收者根据自己的需求从消息Broker那里订阅消息。每当发送方发送消息后，接受者都能获取消息并处理。\n这样，发送消息的人和接收消息的人就完全解耦了。\n就好比送外卖，同步调用就是外卖小哥外卖送到了必须等你在他手上把外卖拿走了才能送下一旦，异步调用就是他送到了，直接把外卖放在外卖柜上（Broker），然后给你发个消息“外卖到了”，你也只用接收“外卖到了”这个消息就好。\n\n针对于刚刚的例子，只用在用户服务完成之后，返回支付服务发送一条消息，然后接下来的交易服务，通知服务，积分服务都只需要监听这个消息即可，他们之间是相互独立的，并发的。\n\n\n不管后期增加了多少消息订阅者，作为支付服务来讲，执行问扣减余额、更新支付流水状态后，发送消息即可，不再需要在代码中增加对其他新的业务的调用。业务耗时仅仅是这三部分业务耗时，仅仅100ms，大大提高了业务性能。\n另外，不管是交易服务、通知服务，还是积分服务，他们的业务与支付关联度低，现在采用了异步调用，解除了耦合，他们即便执行过程中出现了故障，也不会影响到支付服务，解决了级联失败。\n同时有业务功能的添加也不需要动源代码，只需接收他们发出的消息即可，拓展性更好。\n这样的话它的压力主要集中在发送消息这一步，后续的服务可以根据自己的能力按需处理消息，相对来说很平稳，这种现象我们可以理解为削峰。\n\n\n\nMQ综上来看，MQ的本质就是一个阻塞队列， 只不过它在阻塞队列的基础上增加了重试, 消息持久化等等功能.。\n那它有什么优缺点呢？\n我们用去咖啡店买咖啡为例，如果是采用同步通讯的方法去买咖啡，我们给咖啡馆店员点餐后，我们还需要站在柜台等待他把咖啡做完，然后下一位顾客才可以进行点单，整个一条队伍把咖啡都买完的时间非常久。\n\n异步\n\n如果我们在其中运用MQ的思维，在吧台准备一个自动点餐机，用户可以在上面自行点餐，然后生成一张小票，点完之后，用户就可以走了，不用等待店员把咖啡做好，而咖啡馆店员也不用在吧台等别人来点餐，他只需要在后台备料，等有订单了，再去着手做咖啡。这就是异步。\n\n解耦\n\n在点单过后，店员就着手于做咖啡，而顾客也不用站在队伍里等候，顾客可以去上厕所，可以去买早餐，这就达到了解耦的目的。\n\n削峰\n\n在人流量很大的时候，比如早高峰，很多人都需要购买咖啡，然后不同的店员做咖啡的效率不一样，每个顾客点的咖啡做的难度也不一样，店员谁有空谁就去做下一杯，就可以避免一个店员过载导致的等待时间过长，这就是削峰。\n虽然有很多优点，但是这种模式下也会有缺点。\n\n可用性降低\n\n系统不同服务之间就靠一个MQ连接，如果MQ挂了，整个服务器就崩溃了（虽然不太可能）。\n\n复杂度更高\n\n消息的重复出现，消息是否会失效，还有顺序问题都导致了系统复杂的更高。\n\n数据一致性的问题\n\n系统中难免会发生某一个业务出现问题，就比如B,C,D三个服务都需要对同一个事务进行数据库的写入或者修改，B,C都成功了，D失败了，整个事务在数据库中的状态就不一样了，这也是需要考虑的点。\n","categories":["MQ"],"tags":["MQ"]},{"title":"MySQl--事务","url":"/2025/07/31/MySQl-%E4%BA%8B%E5%8A%A1/","content":""},{"title":"MySQL--从架构原理到索引及SQL优化","url":"/2025/07/31/MySQL-%E4%BB%8E%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86%E5%88%B0%E7%B4%A2%E5%BC%95%E5%8F%8ASQL%E4%BC%98%E5%8C%96/","content":"体系结构&#x2F;SQL执行过程\n在实际开发中，我们写下的 SQL 并不会直接拿去执行，而是要经过 MySQL 内部多个组件的协作处理。这就像你发出一个“订单”，需要经过接单、分配、调度、执行等流程，最终才拿到结果。\n\n了解这些执行流程，是我们掌握 SQL 优化、索引调优和事务隔离等高级特性的前提。\n下图为MySQL的体系结构：\n\n\n我们编写的每一条SQL语句都会经过这个流程：\n\n建立连接：使用到MySQL中的连接器\n在真正建立 MySQL 会话之前，操作系统底层要先通过 TCP 协议建立连接，经历三次TCP握手。（这一步与MySQL的连接器无关）；\n校验用户名和密码；\n校验权限；\n初始化会话。\n\n\n查询缓存（MySQL8.0已废弃，因为对于更新比较频繁的表，查询缓存命中率低）：key为SQL语句，value为查询结果。\n分析器：分为词法分析和语法分析，对SQL语句拆分并做如下分析\n词法分析：提取关键字；\n语法分析：语法校验，构建SQL语法树，便于后续模块读取字段。\n\n\n预处理：检验SQL语句中的表和字段是否存在，将select *中的*拓展为表上的所有列。\n优化器：\n表里有多个索引时，决定用哪个索引；\nSQL语句中有多表连接时，决定连接顺序。\n\n\n执行器：调用相应引擎接口执行SQL语句。\n存储引擎（默认为InnoDB)：真实读写数据。\n返回结果。\n\n\nTCP（Transmission Control Protocol） 是传输控制协议，它是一种面向连接、可靠传输、按顺序交付的数据传输协议。\nTCP 在传输前需要先建立连接，这叫三次握手（Three-way Handshake）：\n\n客户端 → 服务端：发送 SYN 请求（我要连你）\n服务端 → 客户端：返回 SYN + ACK（我知道你要连）\n客户端 → 服务端：发送 ACK 确认（我也知道你知道我要连）\n\n\n接下来我们来介绍MySQL中的存储引擎。\n字段执行顺序\nfrom：查询操作首先肯定要先知道是查的哪张表\njoin：和from一起走\nwhere：知道查哪张表后，才能根据条件检索对应的数据，过滤原始行\ngroup by：按照字段分组\nhaving：和聚合函数一起用的话，就执行函数后再过滤数据\nselect：到这数据库再决定返回什么数据\norder by：返回需要的数据后才能排序\nlimit：最后才能截取，才知道你要哪几行\n\n搭建数据基础：from，join\n预处理：where，gruop by，聚合。having\n字段选取和加工：select，distinct\n最后结果整理：order by，limit\n存储引擎对于存储引擎，我们可能比较陌生，但是引擎这个概念倒是家喻户晓。\n对于舰载机、直升机、火箭来说，他们都有各自的引擎，是他们最为核心的组件。而我们在选择引擎的时候，需要在合适的场景，选择合适的存储引擎，就像在直升机上，我们不能选择舰载机的引擎一样。\n存储引擎就是存储数据、建立索引、更新&#x2F;查询数据等技术的实现方式 。存储引擎是基于表的，而不是基于库的，所以存储引擎也可被称为表类型。我们可以在创建表的时候，来指定选择的存储引擎，如果没有指定将自动选择默认的存储引擎。\n\n建表时指定存储引擎：\n\n CREATE TABLE  表名(字段1  字段1类型   [ COMMENT  字段1注释 ] , ......字段n  字段n类型   [COMMENT  字段n注释 ] ) ENGINE = INNODB   [ COMMENT  表注释 ] ;\n\n\n查询当前数据库支持的存储引擎\n\nshow engines;\n\n\n\n里面比较重要的引擎有：InnoDB、MyISAM、Memory。\n\n查询建表语句\n\nshow create table account;\n\n\n\n我们之前建表的时候没有指定引擎，可见默认的存储引擎就是InnoDB。\n上面我们介绍了什么是存储引擎，以及如何在建表时如何指定存储引擎，接下来我们就来介绍下来上面重点提到的三种存储引擎 InnoDB、MyISAM、Memory的特点。\nInnoDBInnoDB是一种兼顾高可靠性和高性能的通用存储引擎，在 MySQL 5.5 之后，InnoDB是默认的MySQL 存储引擎。\n特点：\n\n支持事务；\n行级锁，提高并发访问性能； \n支持外键FOREIGN KEY约束，保证数据的完整性和正确性。\n\n文件：\nxxx.ibd：xxx代表的是表名，innoDB引擎的每张表都会对应这样一个表空间文件，存储该表的表结构（frm-早期的 、sdi-新版的）、数据和索引。\n参数：innodb_file_per_table\nshow variables like &#x27;innodb_file_per_table&#x27;;\n\n\n\n如果该参数开启，代表对于InnoDB引擎的表，每一张表都对应一个ibd文件。 我们直接打开MySQL的 数据存放目录：\n\n\n可以看到里面有很多的ibd文件，每一个ibd文件就对应一张表，比如：我们有一张表 account，就有这样的一个account.ibd文件，而在这个ibd文件中不仅存放表结构、数据，还会存放该表对应的索引信息。\n逻辑存储结构：\n\n\n表空间 : InnoDB存储引擎逻辑结构的最高层，ibd文件其实就是表空间文件，在表空间中可以包含多个Segment段。 \n段 : 表空间是由各个段组成的， 常见的段有数据段、索引段、回滚段等。InnoDB中对于段的管理，都是引擎自身完成，不需要人为对其控制，一个段中包含多个区。 \n区 : 区是表空间的单元结构，每个区的大小为1M。 默认情况下， InnoDB存储引擎页大小为 16K， 即一个区中一共有64个连续的页。 \n页 : 页是组成区的最小单元，页也是InnoDB 存储引擎磁盘管理的最小单元，每个页的大小默 认为 16KB。为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。 \n行 : InnoDB 存储引擎是面向行的，也就是说数据是按行进行存放的，在每一行中除了定义表时所指定的字段以外，还包含两个隐藏字段(后面会详细介绍)。\nMyISAMMyISAM是MySQL早期的默认存储引擎。\n特点： \n\n不支持事务，不支持外键 \n\n支持表锁，不支持行锁 \n\n访问速度快\n\n\nMemoryMemory引擎的表数据时存储在内存中的，由于受到硬件问题、或断电问题的影响，只能将这些表作为临时表或缓存使用。\n特点：\n\n内存存放hash索引（默认），检索速度快，适合做缓存\n表锁\n\n三者区别及特点\n\n\n特点\nInnoDB\nMyISAM\nMemory\n\n\n\n事务安全\n支持\n-\n-\n\n\n锁机制\n行锁\n表锁\n表锁\n\n\n支持外键\n支持\n-\n-\n\n\n\n问：InnoDB引擎与MyISAM引擎的区别 ? \nInnoDB支持事务，外键，行级锁，而MyISAM不支持事务，外键，只有表锁，并发性能低。\nInnoDB支持数据库异常崩溃后的安全恢复，依赖于redo_log，而MyISAM不支持。\nInnoDB支持MVCC，而MyISAM不支持。\n\n\n\n选择InnoDB: 是Mysql的默认存储引擎，支持事务、外键，行级锁，可以保证事务的完整性和高并发性能，所以成为了MySQL的默认引擎。 \nMyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。（MongoDB）\nMEMORY：将所有数据保存在内存中，访问速度快，通常用于临时表及缓存。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，而且无法保障数据的安全性。（Redis）\n索引机制详解\n**索引（index）**是帮助MySQL高效获取数据的数据结构(有序)。在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。\n\n概述索引其实就可以理解成查字典时的目录，在没有目录前，查一个字就是一页页地去翻，这就是全表扫描，用目录就是用索引。\n优势：提高数据检索的效率，降低数据库 的IO成本通过索引列对数据进行排序，降低 数据排序的成本，降低CPU的消耗。 \n劣势：索引列也是要占用空间的。 索引大大提高了查询效率，同时却也降低更新表的速度， 如对表进行INSERT、UPDATE、DELETE时，效率降低。\n基本语法创建索引基本语法如下：\nCREATE  [ UNIQUE | FULLTEXT ]  INDEX  index_name  ON  table_name  ( index_col_name,... ) ;\n\n索引名成规范一般为idx_表名_字段名.\n完成以下要求：\nA. name字段为姓名字段，该字段的值可能会重复，为该字段创建索引。\ncreate index idx_user_name on tb_user(name);\n\n B. phone手机号字段的值，是非空，且唯一的，为该字段创建唯一索引。\ncreate index idx_user_phone on tb_user(phone);\n\nC. 为profession、age、status创建联合索引。 \ncreate index idx_user_pro_age_sta on tb_user(profession,age,status); \n\nD. 为email建立合适的索引来提升查询效率。 \ncreate index idx_email on tb_user(email);\n\n查看索引SHOW  INDEX  FROM  table_name ;\n\n下面查看一下我们刚刚创建的索引：\nshow index from tb_user;\n\n\n\n删除索引DROP  INDEX  index_name  ON  table_name ;\n\n索引分类MySQL的索引是在存储引擎层实现的，不同的存储引擎有不同的索引结构，主要包含以下几种：\n\n\n\n索引结构\n描述\n\n\n\nB+Tree索引\n最常见的索引类型，大部分引擎都支持 B+ 树索引\n\n\nHash索引\n底层数据结构是用哈希表实现的, 只有精确匹配索引列的查询才有效, 不支持范围查询\n\n\nR-tree(空间索 引）\n空间索引是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类 型，通常使用较少。\n\n\nFull-text(全文 索引)\n是一种通过建立倒排索引,快速匹配文档的方式。类似于 Lucene,Solr,ES\n\n\n注意： 我们平常所说的索引，如果没有特别指明，都是指B+树结构组织的索引。\n为什么InnoDb使用B+树索引结构从索引的介绍来看，不难知道，索引就是用来提升检索效率的，那可以实现快速搜索的数据结构有：哈希表，二叉树，红黑树，B树，B+树，接下来我们就一个个分析：\n\nhash表：用唯一的key来查对应的value，虽然很快，但是它没法做范围查询，也没法排序，如下图如果两个(或多个)键值，映射到一个相同的槽位上，他们就产生了hash冲突（也称为hash碰撞），可 以通过链表来解决，\n二叉排序树\n\n左节点小于右节点，右节点大于根，中序遍历是有序的，可以排序也可以范围查询，理想状态下如下图：\n\n\n但是一旦按照主键顺序插入，它就会退化为链表，性能很差如下图：\n\n\n这时你可能会想到二叉平衡树\n\n二叉平衡树：在插入节点的时候，使用旋转使二叉树保证平衡，左右子树高度差不大于1，这样虽然能避免退化为链表，但是由于它会追求绝对平衡，会频繁旋转，在插入数据时，就会带来大量磁盘IO，降低性能。\n红黑树：红黑树也是一种自平衡的二叉排序树，通过插入删除数据时，进行变色和旋转来保持平衡，它不追求绝对平衡，所以大大降低了旋转操作，但是它的本质是二叉树，一个节点只有两个孩子，存储大量数据时，树高很高，性能自然就没那么好。\nB树：它是一种多路平衡排序树，相对于二叉树，B树每个节点可以有多个分支，即多叉，下面展示一个最大度数（max-degree）为5(5阶)的b-tree（每个节点最多存储4个key，5 个指针）：\n\n可以看见它的节点又存索引又存数据，所以一个节点存不了多少索引，数据量很大的时候，树还是会比较高；而且做范围查询时，需要回溯，效率很低。\n\nB+树：它也是多路平衡排序树，而且它的非叶子节点上只存索引值，不存数据，那么单个节点能存的索引值就很多了，树高更低，性能更好；而且B+树会把所有数据存放在最下层叶子节点中，并且由双向链表连接，范围查询只需遍历链表，不用回溯。\n\n\n\n\n那B+树中具体是咋存储数据的呢？\n\n聚集索引 vs 二级索引\n其实MySQL索引可以就分为聚集索引和二级索引（也可称为非聚集索引）。\n\n\n聚集索引：将数据存储与索引放到了一块，索引结构的叶子节点保存了行数据，必须有,而且只有一个（主键索引，由于主键在B+树中，决定了主键就是自增插入，可以避免页分裂带来的性能问题）。\n\n\n聚集索引选取规则: 如果存在主键，主键索引就是聚集索引。 如果不存在主键，将使用第一个唯一（UNIQUE）索引作为聚集索引。 如果表没有主键，或没有合适的唯一索引，则InnoDB会自动生成一个rowid作为隐藏的聚集索引.\n\n\n二级索引：将数据与索引分开存储，索引结构的叶子节点关联的是对应的主键，可以存在多个，在查找的时候先查出当前对应的主键id再回表查出对应的行数据，所以叫二级节点。\n\n二者如下图：\n\n\n接下来，我们来分析一下，当我们执行如下的SQL语句时，具体的查找过程是什么样子的。\nselect * from user where name = &#x27;Arm&#x27;;\n\n\n由于是根据name字段进行查询，所以先根据name&#x3D;’Arm’到name字段的二级索引中进行匹配查 找。但是在二级索引中只能查找到 Arm 对应的主键值 10。\n由于查询返回的数据是*，所以此时，还需要根据主键值10，到聚集索引中查找10对应的记录，最 终找到10对应的行row。\n最终拿到这一行的数据，直接返回即可。\n\n\n思考：InnoDB主键索引的B+tree高度为多高呢?\n\n前面我们可以知道InnoDB的结构如下：\n\n\n然后一个节点占用一页，如下图：\n\n\n\n\n假设: 一行数据大小为1k，那么一页中可以存储16行这样的数据；InnoDB的指针占用6个字节的空间，主键如果为bigint，占用字节数为8。\n高度为2的时候： n * 8 + (n + 1) * 6 = 16*1024 , 算出n约为1170，总共就是1170*16=18720  也就是说，如果树的高度为2，则可以存储 18000 多条记录。\n高度为3： 1171 * 1171 * 16 = 21939856也就是说，如果树的高度为3，则可以存储 2200w 左右的记录。\n这样变相的证明了B+树做为索引结构的优越性\nSQL性能分析\n前面说了这么多关于索引的理论知识，接下来，我们来介绍关于如何得知SQL性能好坏，以便于我们更好的使用索引。\n\n慢日志查询\n慢查询日志记录了所有执行时间超过指定参数（long_query_time，单位：秒，默认10秒）的所有SQL语句的日志。\n\nMySQL的慢查询日志默认没有开启，我们可以查看一下系统变量 slow_query_log，判断是否开启\nshow variables like &#x27;slow_query_log&#x27;;\n\n\n\n可以临时开启慢查询日志：\nset global slow_query_log=&#x27;ON&#x27;;\n\n也可以更改配置文件增加以下两条配置文件开启：\n# 开启MySQL慢日志查询开关slow_query_log=1 # 设置慢日志的时间为2秒，SQL语句执行时间超过2秒，就会视为慢查询，记录慢查询日志long_query_time=2\n\n配置过后一定要重启MySQL服务。\n可以使用以下代码查看慢日志存储在哪\nshow variables like &#x27;slow_query_log_file&#x27;;\n\n\n\n\n\n我们提前准备了一个千万数据表tb_sku，来试着执行一条SQL\nselect count(*) from tb_sku;\n\n\n\n可以看见日志里面就记录了这条sql语句。\nproile的使用\nshow profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪里去了。\n\n通过have_profiling 参数，能够看到当前MySQL是否支持profile操作\nselect @@have_profiling;\n\n\n\n如果没有开启，可以通过以下指令开启：\nSET profiling = 1;\n\n开关已经打开了，接下来，我们所执行的SQL语句，都会被MySQL记录，并记录执行时间消耗到哪儿去 了。\n 我们直接执行如下的SQL语句：\nselect * from tb_user;select * from tb_user where id = 1;select * from tb_user where name = &#x27;白起&#x27;;select count(*) from tb_sku;\n\n执行一系列的业务SQL的操作，然后通过如下指令查看指令的执行耗时:\nshow profiles; -- 查看每一条SQL的耗时基本情况\n\n\n\nshow profile for query query_id;-- 查看指定query_id的SQL语句各个阶段的耗时情况\n\n可以看见具体的执行耗时，最长的就是执行：\n\n\nshow profile cpu for query query_id;  -- 查看指定query_id的SQL语句CPU的使用情况\n\nexplain的使用\nEXPLAIN 或者 DESC命令获取 MySQL 如何执行 SELECT 语句的信息，包括在 SELECT 语句执行 过程中表如何连接和连接的顺\n\n-- 直接在select语句之前加上关键字 explain / descEXPLAIN SELECT 字段列表 FROM 表名 WHERE 条件；\n\n准备三个表，我们来逐步演示每个字段的含义：\n\n\n他们之间的关系如下：\n\n\nid-- 查询每个学生的选课情况explain select s.*,c.* from student s,course c,student_course sc where s.id=sc.studentid and c.id=sc.courseid;\n\n\nid如果是相同的，从上而下的执行，如图就是先查s，后查sc，最后c\n\n\n\nexplain select * from student s where s.id in (select studentid from student_course sc where sc.courseid = (select C.id from course c where c.name = &#x27;MySQL&#x27;));\n\n\nid不同大的先执行\n\n\n\nselect_type\n表示 SELECT 的类型，参考意义不大\n\ntype\n表示连接类型，性能由好到差的连接类型为NULL、system、const、 eq_ref、ref、range、 index、all \n\n一般来说到const就是最好的了;\nnull一般就是你给它什么，它查出什么，比如：\n\n\nsystem就是访问系统表；\nconst就是访问主键或者索引：\n\n\nref是非唯一性possible_key索引：\n\n\npossible_key\n顾名思义就是可能用到的索引。\n\nkey\n实际使用的索引，如果为NULL，则没有使用索引。\n\nkey_len\n表示索引中使用的字节数， 该值为索引字段最大可能长度，并非实际使用长 度，在不损失精确性的前提下， 长度越短越好 。\n\nrows\nMySQL认为必须要执行查询的行数，在innodb引擎的表中，是一个估计值， 可能并不总是准确的。\n\nfiltered\n表示返回结果的行数占需读取行数的百分比， filtered的值越大越好。\n\nExplain 执行计划中比较重要字段的含义:\n\n\n字段\n含义\n\n\n\ntype\n表示连接类型\n\n\npossible_key\n显示可能应用在这张表上的索引，一个或多个。\n\n\nkey\n实际使用的索引，如果为NULL，则没有使用索引。\n\n\nkey_len\n表示索引中使用的字节数， 该值为索引字段最大可能长度，并非实际使用长 度，在不损失精确性的前提下， 长度越短越好 。\n\n\n索引实战接下俩我们开始实际探讨索引的使用及其一些特性。\n验证索引效率我们查询我们之前创建的千万行表的一条没有索引的数据，可以看见尽管只有一条数据，它也查询了15s之久。\n\n\n现在我们给它创建一个索引\ncreate index idx_sku_sn on tb_sku(sn);\n\n可以看见有了索引之后，它几乎是瞬间查询出了数据：\n\n\n最左前缀原则\n如果索引了多列（联合索引），要遵守最左前缀法则。\n\n最左前缀法则指的是查询从索引的最左列开始， 并且不跳过索引中的列。如果跳跃某一列，索引将会部分失效(后面的字段索引失效)。\n我们上面创建了一个联合索引idx_user_pro_age_sta\n\n\n对于最左前缀法则指的是，查询时，最左变的列，也就是profession必须存在，否则索引全部失效。  \n而且中间不能跳过某一列，否则该列后面的字段索引将失效；接下来，我们来演示几组案例，看一下具体的执行计划：\nexplain select * from tb_user where profession = &#x27;软件工程&#x27; and age = 31 and status = &#x27;0&#x27;;\n\n\n可以看见成功使用索引了，而且索引长度为54。\n\n\n\n\n-- 我们删除最后一个字段explain select * from tb_user where profession = &#x27;软件工程&#x27; and age = 31;\n\n\n也使用了索引，并且长度为49，status索引长度应该为5。\n\n\n\n\n-- 我们再删除一个字段explain select * from tb_user where profession = &#x27;软件工程&#x27;;\n\n\n依旧使用了索引，长度为47，age索引长度为2，profession索引长度为47.\n\n\n\n\n-- 我们只是用age和status查询explain select * from tb_user where age = 31 and status = &#x27;0&#x27;;\n\n\n因为没有最左边的字段，所以就没有使用索引，索引失效。\n\n\n\n\n-- 那如果我们使用profession和status呢?explain select * from tb_user where profession = &#x27;软件工程&#x27; and status = &#x27;0&#x27;;\n\n\n可以看见也使用了索引，索引长度为profession的长度47，并没有status的索引，只是部分走了索引。\n\n\n\n\n-- 再执行以下代码，是否会走索引呢？explain select * from tb_user where age = 31 and status = &#x27;0&#x27; and profession = &#x27;软件工程&#x27;；\n\n\n可以看见是走索引了，而且所有索引都走了，由此可见与条件的排列顺序其实是没有关系的。\n\n\n\n索引失效场景汇总\n索引不是在任何时候都会发生作用的，以下我们就会介绍很多种索引失效的场景。\n\n最左前缀法则\n上面已经讲的很清楚了，这里不过多介绍。\n\n范围查询\n联合索引中，出现范围查询(&gt;,&lt;)，范围查询右侧的列索引失效。\n\n接下来演示一下：\nexplain select * from tb_user where profession = &#x27;软件工程&#x27; and age &gt; 30 and status = &#x27;0&#x27;;\n\n\n走了索引，为profession和age的索引，因为status的在范围查询的右边，所以失效了。\n\n\n\n\n有没有规避的方法呢？\n\n有的，就是在业务允许的情况下，只使用类似于 &gt;&#x3D; 或 &lt;&#x3D; 这类。\n\n我们在把&gt;改成&gt;=，可以发现所有索引都被使用了。\n\n\n\n索引列运算&#x2F;函数处理\n当根据phone字段进行等值匹配查询时, 索引生效。\n\nexplain select * from tb_user where phone = &#x27;17799990015&#x27;;\n\n\n\n\n当根据phone字段进行函数运算操作之后，索引失效。\n\nexplain select * from tb_user where substring(phone,10,2) = &#x27;15&#x27;;\n\n\n\n字段类型不匹配\n字符串类型字段使用时，不加引号，索引将失效。\n\nexplain select * from tb_user where phone = &#x27;17799990015&#x27;;\n\n\n\nexplain select * from tb_user where phone = 17799990015;\n\n\n\n经过上面两组示例，我们会明显的发现，如果字符串不加单引号，对于查询结果，没什么影响，但是数据库存在隐式类型转换，索引将失效。\n模糊查询\n如果仅仅是尾部模糊匹配，索引不会失效。如果是头部模糊匹配，索引失效。\n\n由于下面查询语句中，都是根据profession字段查询，符合最左前缀法则，联合索引是可以生效的， 我们主要看一下，模糊查询时，%加在关键字之前，和加在关键字之后的影响。\nexplain select * from tb_user where profession like &#x27;软件%&#x27;;\n\n\n\nexplain select * from tb_user where profession like &#x27;%工程&#x27;;\n\n\n\nexplain select * from tb_user where profession like &#x27;%工%&#x27;;\n\n\n\n经过上述的测试，我们发现，在like模糊查询中，在关键字后面加%，索引可以生效。而如果在关键字 前面加了%，索引将会失效。\nor连接条件\n用or分割开的条件， 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。\n\n下面的age没有索引，phone有索引。\nexplain select * from tb_user where id = 10 or age = 23;\n\n\n\nexplain select * from tb_user where phone = &#x27;17799990017&#x27; or age = 23;\n\n\n由于age没有索引，所以即使id、phone有索引，索引也会失效。所以需要针对于age也要建立索引。\n\n\n\n然后，我们可以对age字段建立索引，重新执行：\n\n\n\n\n最终，我们发现，当or连接的条件，左右两侧字段都有索引时，索引才会生效。\n数据分布\n如果MySQL评估使用索引比全表更慢，则不使用索引。\n\ntb_user表单数据如下：\n\n\n\n首先查询基本整个表的数据\n\nexplain select * from tb_user where phone &gt;= &#x27;17799990005&#x27;;\n\n\n可以看见走到是全表查询，这是因为MySQL自己判断，认为走全表查询比走索引更快。\n\n\n\n\n然后我们只查询后面几行数据\n\nexplain select * from tb_user where phone &gt;= &#x27;17799990015&#x27;;\n\n\n这时可以看见它走的就是索引了。\n\n\n\n经过测试我们发现，相同的SQL语句，只是传入的字段值不同，最终的执行计划也完全不一样，这是为什么呢？ \n就是因为MySQL在查询时，会评估使用索引的效率与走全表扫描的效率，如果走全表扫描更快，则放弃索引，走全表扫描。 \n因为索引是用来索引少量数据的，如果通过索引查询返回大批量的数据，则还不如走全表扫描来的快，此时索引就会失效。\nSQL提示把上述的 idx_user_age, idx_email 这两个之前测试使用过的索引直接删除。\n然后我们创建profession的单列索引：\ncreate index idx_user_pro on  tb_user(profession);\n\n执行SQL :\nexplain select * from tb_user where profession = &#x27;软件工程&#x27;;\n\n\n\n\n发现走的联合索引，我们可以看到，possible_keys中idx_user_pro_age_sta,idx_user_pro 这两个 索引都可能用到，最终MySQL选择了idx_user_pro_age_sta索引。这是MySQL自动选择的结果。\n\n那么，我们能不能在查询的时候，自己来指定使用哪个索引呢？ 答案是肯定的，此时就可以借助于 MySQL的SQL提示来完成。 接下来，介绍一下SQL提示。\n\nSQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。 \n\n\nuse index ： 建议MySQL使用哪一个索引完成此次查询（仅仅是建议，mysql内部还会再次进 行评估）\nignore index ： 忽略指定的索引\nforce index ： 强制使用索引\n\n示例\nexplain select * from tb_user use index(idx_user_pro) where profession = &#x27;软件工程&#x27;;\n\n\n\nexplain select * from tb_user ignore index(idx_user_pro) where profession = &#x27;软件工程&#x27;;\n\n\n\nexplain select * from tb_user force index(idx_user_pro_age_sta) where profession = &#x27;软件工程&#x27;;\n\n\n\n覆盖索引与回表\n覆盖索引是指查询使用了索引，并且需要返回的列，在该索引中已经全部能够找到 。\n\nexplain select id,profession,age, status from tb_user  where profession = &#x27;软件工程&#x27; and age = 31 and status = &#x27;0&#x27;;\n\n\nUsing where; Using Index：查找使用了索引，但是需要的数据都在索引列中能找到，所以不需要回表查询数据\n\n\n\n\nexplain select id,profession,age, status, name from tb_user  where profession = &#x27;软件工程&#x27; and age = 31 and status = &#x27;0&#x27;;\n\n\nUsing index condition：查找使用了索引，但是需要回表查询数据\n\n\n\n\nUsing where; Using index 比Using index condition的性能高\n因为，在tb_user表中有一个联合索引idx_user_pro_age_sta，该索引关联了三个字段  profession、age、status，而这个索引也是一个二级索引，所以叶子节点下面挂的是这一行的主键id。\n所以当我们查询返回的数据在id、profession、age、status 之中，则直接走二级索引直接返回数据了。\n如果超出这个范围，就需要拿到主键id，再去扫描聚集索引，再获取额外的数据 了，这个过程就是回表。\n而我们如果一直使用select * 查询返回所有字段值，很容易就会造成回表查询（除非是根据主键查询，此时只会扫描聚集索引）。\n\n根据id查询，直接走聚集索引查询，一次索引扫描，直接返回数据，性能高。\n\n\n\n\n执行SQL：selet id,name from tb_user where name &#x3D; ‘Arm’;\n\n\n\n\n\n\n执行SQL：selet id,name,gender from tb_user where name &#x3D; ‘Arm’\n\n\n\n由于在name的二级索引中，不包含gender，所以，需要两次索引扫描，也就是需要回表查询，性能相对较差一点。\n\n一张表, 有四个字段(id, username, password, status), 由于数据量大, 需要对 以下SQL语句进行优化, 该如何进行才是最优方案:\n\nselect id,username,password from tb_user where username = &#x27;itcast&#x27;; \n\n\n答案: 针对于username, password建立联合索引, sql为:\n\ncreate index  idx_user_name_pass on tb_user(username,password); \n\n\n这样可以避免上述的SQL语句出现回表查询。\n\n尽量使用覆盖索引，减少select *。 \n前缀索引\n当字段类型为字符串（varchar，text，longtext等）时，有时候需要索引很长的字符串，这会让索引变得很大，查询时，浪费大量的磁盘IO， 影响查询效率；此时可以只将字符串的一部分前缀，建 立索引，这样可以大大节约索引空间，从而提高索引效率。\n\n语法：\ncreate index idx_xxxx on table_name(column(n)) ;\n\n\n前缀长度\n\n可以根据索引的选择性来决定，而选择性是指不重复的索引值（基数）和数据表的记录总数的比值， 索引选择性越高则查询效率越高， 唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。\nselect count(distinct email)/count(*) from tb_user;\n\n\n\n\n我们使用substring函数，不断减少索引的长度，尽量让它又短，同时选择性又尽量大，最后发现截取到前五位是最好的。\n\nselect count(distinct substring(email,1,5))/count(*) from tb_user;\n\n\n\n这时我们就可以创建索引：\ncreate index idx_email_5 on tb_user(email(5));\n\n\n\n前缀索引的查询流程：\n\n\nstep1：先在当前前缀索引中查询到这行数据的id\nstep2：回表查询到一整行的数据\nstep3：在这一行数据的email的值与条件对比，然后返回这个值\n最后在前缀索引当中看下一个值是否对应的上，如果可以重复上述操作，直至没有相同的前缀为止\n其实前缀索引就是一个时间换空间的过程。\n单列&amp;联合索引\n单列索引：即一个索引只包含单个列。 联合索引：即一个索引包含了多个列。\n\n我们先来看看tb_user表中目前的索引情况:\n\n\n我们针对于name和phone进行查询：\nexplain select name,phone from tb_user where name =&#x27;吕布&#x27; and phone =&#x27;17799990000&#x27;;\n\n\n可以看见实际使用的只有一个索引，此时是会回表查询的\n\n\n\n紧接着，我们再来创建一个phone和name字段的联合索引来查询一下执行计划。\ncreate unique index idx_user_phone_name on tb_user(phone,name);\n\n\n可以看见就没有回表查询了\n\n\n\n多个单列索引的组合在混合条件下查询是会进行回表的，这种时候我们既可以使用联合索引来优化。\n索引下推（ICP)\n索引下推就是把 WHERE 条件尽量“推”到索引扫描阶段，省掉无用回表：可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，然后再去做回表，从而减少了回表次数，提升了性能。\n\n假设索引为  (name, age)，查询语句如下：\nSELECT * FROM user WHERE name LIKE &#x27;王%&#x27; AND age = 30;\n\n联合索引先按name排序，name一样再按age排序，如果是 name&#x3D;”张三” and age &gt; 18，这个就能使用联合索引的所有列. 不需要索引下推。\n\n无ICP\n\n这里走联合索引先筛选出姓名以王开头的用户\n由于这里是模糊匹配，不是等值匹配，故获取所有以王开头的用户后, 他们的age不一定有序的\n所以无法继续使用联合索引的特性来筛选age, 只能拿到以王开头的用户的id, 去回表, 然后再筛选出age&#x3D;30的人.\n\n\n有ICP\n\n走联合索引先筛选出姓名以王开头且同时age&#x3D;30的用户\n回表只需要根据他的id查即可\n\n\n\n可以看到使用了索引下推后, 大大减少了回表操作。\n索引设计原则\n针对于数据量较大，且查询比较频繁的列建立索引。 \n\n针对于常作为查询条件（where）、排序（order by）、分组（group by）操作的字段建立索引。\n\n尽量选择区分度高的列作为索引，例如身份证号建议建立索引，但是性别不建议。\n\n字符串字段较长，可以建立前缀索引。 \n\n尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以覆盖索引，避免回表，提高查询效率。\n\n要控制索引的数量，索引并不是多多益善，索引越多，维护索引结构的代价也就越大，会影响增删改的效率。\n\n\nSQL优化\n上面我们了解了索引的机制，以及使用原则，其实不难看出，索引就是为了优化select查询语句的，那剩下的SQL如何优化呢？下面就开始探究其他SQL的优化方法。\n\n插入数据insert如果我们需要一次性往数据库表中插入多条记录，可以从以下三个方面进行优化。\ninsert  into  tb_test  values(1,&#x27;tom&#x27;);insert  into  tb_test  values(2,&#x27;cat&#x27;);insert  into  tb_test  values(3,&#x27;jerry&#x27;);....\n\n批量插入Insert  into  tb_test  values(1,&#x27;Tom&#x27;),(2,&#x27;Cat&#x27;),(3,&#x27;Jerry&#x27;)\n\n如果数据量很大一条sql插入的数据最好不要超过1000条；往往来说500-1000比较合理。\n手动控制事务由于mysql中的事务提交方式是自动提交的，也就意味着执行一条sql语句后，它就会自动提交事务，频繁开启事务也会影响性能。\n这时我们就可以手动管理事务的开启关闭，在多条sql执行之后再提交事务。\nstart  transaction;insert  into  tb_test  values(1,&#x27;Tom&#x27;),(2,&#x27;Cat&#x27;),(3,&#x27;Jerry&#x27;);insert  into  tb_test  values(4,&#x27;Tom&#x27;),(5,&#x27;Cat&#x27;),(6,&#x27;Jerry&#x27;);commit\n\n主键顺序插入主键顺序插入，性能要高于乱序插入（详细在后面主键优化）。 \n主键乱序插入 : 8  1  9  21  88  2  4  15  89  5  7  3   主键顺序插入 : 1  2  3  4  5  7  8  9  15  21  88  89\n\nload在批量插入大量数据的时候，insert就不那么好用了，这个时候，我们就要使用load指令来批量插入：\nload data local infile &#x27;/.../..&#x27; into table tb_user fields terminated  by  &#x27;,&#x27;  lines terminated by &#x27;\\n&#x27; ;\n\n这是基本语法格式：这串命令的含义是把本地/.../..路径中的文件，每一个元素按照,分隔，每一行又换行分隔，最后插入到tb_user中。\n其中的形式不定，可以根据实际情况改变，例子中的格式为：\n\n\n命令行中不管是cmd还是linux要使用load指令，首先必须要开启local_infile：\n\n直接用local_infile=1登录\n\nmysql --local-infile=1 -u root -p\n\n\n登录时附加local_infile，然后将其设置为1\n\nmysql --local-infile -u root -p\n\nset global local_infile = 1;\n\n\n\n可以用以下指令查看是否修改成功：\nselect @@local_infile;\n\ndatagrip右键连接，选择属性(properties)，在右边的高级（advanced），将allowLoadLoaclInfile设置为true。\n\n\n演示最后插入一百万条数据用时14s，如果用insert要用十分钟左右，大大提升了性能。\n\n\n主键优化在上一小节，我们提到，主键顺序插入的性能是要高于乱序插入的。 这一小节，就来介绍一下具体的 原因，然后再分析一下主键又该如何设计。\n在InnoDB存储引擎中，表数据都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表。\n\n\n前面我们知道InnoDB的逻辑结构，数据行是记录在逻辑结构 page 页中的，而每一个页的大小是固定的，默认16K。\n那也就意味着，一个页中所存储的行也是有限的，如果插入的数据行row在该页存储不小，将会存储到下一个页中，页与页之间会通过指针连接。\n主键顺序插入\n从磁盘中申请页， 主键顺序插入\n\n\n\n\n第一个页没有满，继续往第一页插入\n\n\n\n\n当第一个也写满之后，再写入第二个页，页与页之间会通过指针连接\n\n\n\n后续反复。\n乱序插入导致页分裂\n加入1#,2#页都已经写满了，存放了如图所示的数据\n\n\n\n\n此时再插入id为50的记录，我们来看看会发生什么现象\n\n由于索引结构的叶子节点是有顺序的。按照顺序，应该存储在47之后\n\n\n\n\n但是47所在的1#页，已经写满了，存储不了50对应的数据了。 那么此时会开辟一个新的页 3#。\n\n\n\n\n但是并不会直接将50存入3#页，而是会将1#页后一半的数据，移动到3#页，然后在3#页，插入50。\n\n\n\n移动数据，并插入id为50的数据之后，那么此时，这三个页之间的数据顺序是有问题的。 1#的下一个 页，应该是3#， 3#的下一个页是2#。 所以，此时，需要重新设置链表指针\n\n\n\n\n上述的这种现象，称之为 “页分裂”，是比较耗费性能的操作。\n页合并目前表中已有数据的索引结构(叶子节点)如下：\n\n\n\n当删除一行记录时，实际上记录并没有被物理删除，只是记录被标记（flaged）为删除并且它的空间 变得允许被其他记录声明使用。\n\n\n\n\n当页中删除的记录达到 MERGE_THRESHOLD（默认为页的50%），InnoDB会开始寻找最靠近的页（前或后）看看是否可以将两个页合并以优化空间使用\n\n\n\n\n\n\n删除数据，并将页合并之后，再次插入新的数据21，则直接插入3#页\n\n\n\n这个里面所发生的合并页的这个现象，就称之为 “页合并”。\n\n**MERGE_THRESHOLD：**合并页的阈值，可以自己设置，在创建表或者创建索引时指定。\n\n主键优化的原则\n满足业务需求的情况下，尽量降低主键的长度。 \n插入数据时，尽量选择顺序插入，选择使用AUTO_INCREMENT自增主键。 \n尽量不要使用UUID做主键或者是其他自然主键，如身份证号。 \n业务操作时，避免对主键的修改\n\norder by优化实践MySQL的排序，有两种方式：\n\nUsing filesort : 通过表的索引或全表扫描，读取满足条件的数据行，然后在排序缓冲区sort  buffer中完成排序操作，所有不是通过索引直接返回排序结果的排序都叫FileSort 排序。 \n\nUsing index : 通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。\n\n\n我们首先只保留以下索引：\n\n\n\n只对age和phone升序排列\n\nexplain select id,age,phone from tb_user order by age;\n\n\n\nexplain select id,age,phone from tb_user order by age，phone;\n\n\n\n\n因为他们都没有索引，所以都是Using filesort，性能不好。\n\n\n我们给他们创建联合索引\n\ncreate index idx_user_age_phone on tb_user(age,phone); #默认两个升序\n\n再次执行：\n\n\n\n\n\n发现两次排序都是Using index，性能好。\n\n\n如果我们对age，phone均降序排列\n\nexplain select id,age,phone from tb_user order by age desc,phone desc;\n\n\n\n\n也是Using index，使用了反向扫描索引。\n\n\n如果我们先对phone升序排列，相同时再对age升序排列\n\nexplain select id,age,phone from tb_user order by phone,age;\n\n\n\n\n违背最左前缀法则，没有使用索引。(索引是age在前，是age先排，这里先给phone排压根找不到索引，就是最左前缀法则)\n\n\n如果我们先对age升序排列，再对phone降序排列\n\nexplain select id,age,phone from tb_user order by age asc ,phone desc;\n\n\n\n\n使用了索引，但是没有完全使用，性能没有提升。\n\n为什么呢？\n查看索引，可以看见他们两个字段的值都是A，表示是升序索引。\n\n\n\n不难想到，创建索引的时候age和phone是绑定在一起的，先通过升序排列确定了age的顺序，但是phone要降序排序就会重新搜索排序，自然就没有使用到索引来提升性能了。\n\n那我们是否可以通过优化就让它可以降序排列呢？\n是可以的，我们可以指定对应字段是升序还是降序。\ncreate index idx_user_age_pho_ad on tb_user(age asc,phone desc );\n\n\n\n\n可以看见phone就成了降序索引。\n\n再次执行可以看见使用索引成功提升到了性能。\n\n\n附：索引的结构\n\n\n\norder by优化原则\n根据排序字段建立合适的索引，多字段排序时，也遵循最左前缀法则。 \n尽量使用覆盖索引。 \n多字段排序, 一个升序一个降序，此时需要注意联合索引在创建时的规则（ASC&#x2F;DESC）。 \n如果不可避免的出现filesort，大数据量排序时，可以适当增大排序缓冲区大小  sort_buffer_size(默认256k)。\n\n\n\ngruop by优化实践\n先删除掉所有索引。\n\nexplain select profession, count(*) from tb_user group by profession;\n\n\n\n\n可以看见性能是不好的，没有使用到索引\n\n然后，我们在针对于 profession ， age， status 创建一个联合索引。\ncreate index idx_user_pro_age_sta on tb_user(profession,age,status);\n\n\n\n\n加上联合索引后，增强了性能。\n\n\n-- 只使用age组合explain select age, count(*) from tb_user group by age;\n\n\n\n\n使用索引失败，因为违反了最左前缀法则。\n\n\n-- 不使用statusexplain select profession,age, count(*) from tb_user group by profession,age;\n\n\n\n\n也使用了索引，与最左前缀法则一致。\n\n\n-- 先指定profession，再对age组合explain select age, count(*) from tb_user where profession = &#x27;软件工程&#x27; group by age;\n\n\n\n\n这样也使用了索引，因为age在profession后面，profession相同的情况下，age就是有序的。\n\n总结所以，在分组操作中，我们需要通过以下两点进行优化，以提升性能： \n\n在分组操作时，可以通过索引来提高效率。\n分组操作时，索引的使用也是满足最左前缀法则的。\n\nlimit优化\n在数据量比较大时，如果进行limit分页查询，在查询时，越往后，分页查询效率越低。\n\n实践\n\n\n通过测试我们会看到，确实越往后，分页查询效率越低，这就是分页查询的问题所在\n\n优化思路: 一般分页查询时，通过创建覆盖索引能够比较好地提高性能，可以通过覆盖索引加子查询形式进行优化。\n\n我们先用order by做为踏板使用主键索引查询id\n\nselect id from tb_sku order by id limit 9000000,10;\n\n\n\n\n我们知道limit是在order by后执行的，先用order by对索引的检索，快速找到9000000，然后再去使用limit。\n\n\n嵌合select *\n\nselect * from tb_sku where id in (select id from tb_sku order by id limit 9000000,10);\n\n\n\n\n不支持，那只有使用连表查询了。\n\nselect a.* from tb_sku a, (select id from tb_sku order by id limit 9000000,10) b where a.id = b.id;\n\n\n\n提升了近50%的效率，还是很可观了。\n总结\n通过覆盖索引查询主键id\n再通过连表查询的方式回表查询行数据\n\ncount优化\n在之前的测试中，我们发现，如果数据量很大，在执行count操作时，是非常耗时的。 \n\n\nMyISAM引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个 数，效率很高； 但是如果是带条件的count，MyISAM也慢。\n\nInnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出 来，然后累积计数。\n\n\n如果说要大幅度提升InnoDB表的count效率，主要的优化思路：自己计数(可以借助于redis这样的数 据库进行,但是如果是带条件的count又比较麻烦了)。\ncount用法count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加 1，否则不加，最后返回累计值。\n用法：count（*）、count（主键）、count（字段）、count（数字）\n\n\n\n用法\n含义\n\n\n\ncount(主 键)\nInnoDB 引擎会遍历整张表，把每一行的主键id值都取出来，返回给服务层。 服务层拿到主键后，直接按行进行累加(主键不可能为null)。\n\n\ncount(字 段)\n1.没有not null约束 : InnoDB引擎会遍历整张表把每一行的字段值都取出来，返回给服务层，服务层判断是否为null，不为null，计数累加。2.有not null 约束：InnoDB引擎会遍历整张表把每一行的字段值都取出来，返 回给服务层，直接按行进行累加。\n\n\ncount(数 字)\nInnoDB引擎遍历整张表，但不取值。服务层对于返回的每一行，放一个数字“1” 进去，直接按行进行累加。\n\n\ncount(*)\nInnoDB引擎并不会把全部字段取出来，而是专门做了优化，不取值，服务层直接按行进行累加。\n\n\n按照效率排序的话，count(字段) &lt; count(主键 id) &lt; count(1) ≈ count()，所以尽 量使用 count()。\n总结尽量使用count(*)。\nupdate优化\nInnoDB是行级锁，是对索引加的锁，不是针对记录加的锁： 如果where条件是不带索引的字段,，那么就会是表锁.；如果where条件是带索引的字段, 那么是行锁. 并且该索引不能失效, 否则会从行锁升级为表锁。表锁的并发性能低。\n\n实践先看一下我们的course表结构：\n\n\n然后针对于这表创建两个会话：\n\n\n\n两会话均是where有索引的更新\n\n-- 会话1begin;update course set name = &#x27;JavaEE&#x27; where id =1;commit;\n\n\n\n-- 会话2begin;update course set name = &#x27;MQ&#x27; where id = 4;commit\n\n\n\n\n他们是可以同时更新数据的，互不影响，并发性能好。\n\n\n\n针对于没有索引的name字段更新\n\n-- 会话1begin;update course set name = &#x27;SpringBoot&#x27; where name = &#x27;PHP&#x27;;commit;\n\n\n\n-- 会话2begin;update course set name = &#x27;Kafka&#x27; where id = 4;commit;\n\n\n\n\n\n\n可以看见会话1执行过后，由于没有索引，所以开启了表锁，会话2直接无法更新，被卡住了进程。\n\n\n创建name索引重新执行\n\ncreate index idx_course_name on course(name);\n\n\n\n\n\n\n是没有问题的。\n\n总结执行更新操作时，where条件尽量用有索引的字段。\nwhere优化\n同前面索引失效的情况，总结如下：\n\n应尽量避免在 where 子句中使用!&#x3D;或&lt;&gt;操作符，否则将引擎放弃使 用索引而进行全表扫描。\n","categories":["MySQL"],"tags":["MySQL"]},{"title":"MySQL--视图、存储过程及触发器","url":"/2025/08/04/MySQL-%E8%A7%86%E5%9B%BE%E3%80%81%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%8F%8A%E8%A7%A6%E5%8F%91%E5%99%A8/","content":"视图\n视图（View）是一种虚拟存在的表。视图中的数据并不在数据库中实际存在，行和列数据来自定义视 图的查询中使用的表，并且是在使用视图时动态生成的。 \n通俗的讲，视图只保存了查询的SQL逻辑，不保存查询结果。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。\n\n语法创建\n创建\n\nCREATE [OR REPLACE] VIEW 视图名称[(列名列表)] AS SELECT语句 [WITH [CASCADED|LOCAL] CHECK OPTION ]\n\n现在我们基于表student建立一个视图（这里有个参数是or replace，表示是否覆盖原来的视图，创建的时候可以不加）\n-- 创建视图create or replace view stu_v_1 as select id,name from student where id &lt;=10;\n\n\n\n查询-- 查看创建视图语句SHOW CREATE VIEW 视图名称;-- 查看视图数据SELECT * FROM 视图名称 ...... ;\n\n修改-- 方式一CREATE [OR REPLACE] VIEW 视图名称[(列名列表)] AS SELECT语句 [WITH [CASCADED|LOCAL] CHECK OPTION]-- 方式二ALTER VIEW 视图名称[(列名列表)] AS SELECT语句 [WITH[CASCADED|LOCAL] CHECK OPTION]\n\n这里修改视图的第一种方式就必须携带or replace了。\n删除DROP VIEW [IF EXISTS] 视图名称 [,视图名称] ... \n\n\n检查选项我们来试试向视图中插入数据：\ninsert into stu_v_1 values(6,&#x27;Tom&#x27;);insert into stu_v_1 values(17,&#x27;Tom22&#x27;);\n\n在基表当中，我们可以发现两条数据都插入了：\n\n\n但是视图当中只有id为6的：\n\n\n\n因为我们在创建视图的时候，是相当于把数据先插入到基表中，然后再重新创建一个视图，这时会重新判断条件，指定的条件为 id&lt;&#x3D;10, id为17的数据，是不符合条件的，所以没有查询出来。\n\n如果我们定义视图时，如果指定了条件，然后我们在插入、修改、删除数据时，是否可以做到必须满足条件才能操作，否则不能够操作呢？ \n\n答案是可以的，这就需要借助于视图的检查选项了。\n\n当使用WITH CHECK OPTION子句创建视图时，MySQL会通过视图检查正在更改的每个行，例如 插 入，更新，删除，以使其符合视图的定义。 \nMySQL允许基于另一个视图创建视图，它还会检查依赖视图中的规则以保持一致性。\n为了确定检查的范围，mysql提供了两个选项： CASCADED 和 LOCAL  ，默认值为 CASCADED 。\n我们带上检查选项再次创建视图：\ncreate or replace view stu_v_1 as select id,name from student where id&lt;=10 with check option;\n\n这时我们再次插入刚才的数据，发现6还是不影响，但是17就插入失败了：\n\n\n\n这是因为加上检查选项之后，系统就会在你每次更新视图时，去检查行的条件是否符合创建视图时的条件。\n\n我们知道检查选项有两个参数，现在我们就来分别对其进行剖析。\nCASCADED\n级联。 \n比如，v2视图是基于v1视图的，如果在v2视图创建的时候指定了检查选项为 cascaded，但是v1视图 创建时未指定检查选项。 则在执行检查时，不仅会检查v2，还会级联检查v2的关联视图v1，如下图：\n\n我们首先创建一个不带检查选项的视图\ncreate or replace view stu_v_1 as select id,name from student where id &lt;=20\n\n它的插入都可以成功，因为它没有带检查选项。\n我们再基于stu_v_1创建一个带cascaded的视图：\ncreate or replace view stu_v_2 as select id,name from stu_v_1 where id &gt;=10 with cascaded check option ;\n\n这时我们再来插入几组数据：\n\nid为6\n\ninsert into stu_v_2 values (6,&#x27;Tom&#x27;);\n\n\n插入失败，由于不满足它本身的条件\n\n\n\n\nid为13\n\ninsert into stu_v_2 values (13,&#x27;Tom&#x27;);\n\n\n插入成功\n\n\n\n\nid为21\n\ninsert into stu_v_2 values (21,&#x27;Tom&#x27;);\n\n\n插入失败，为什么呢？\n明明21满足它本身的条件（id&gt;&#x3D;10），这是因为它是基于stu_v_1的视图，同时又携带cascaded的检查选择，它会将这个设置传递给stu_v_1，让stu_v_1也会检查，整体呈现一种递归性。\n\n\n\n\n我们再基于stu_v_2创建一个不带检查选项的视图：\ncreate or replace view stu_v_3 as select id,name from stu_v_2 where id &lt;=15;\n\n我们插入几条数据：\n\nid为11和17\n\ninsert into stu_v_2 values (11,&#x27;Tom&#x27;);insert into stu_v_2 values (17,&#x27;Tom&#x27;);\n\n\n他们都可以插入成功，因为stu_v_3本身没有条件，所以不管满不满足它自己，都不会触发检查\n\n\n\n\nid为7和28\n\ninsert into stu_v_2 values (7,&#x27;Tom&#x27;);insert into stu_v_2 values (28,&#x27;Tom&#x27;);\n\n\n这两条数据插入失败了，是为什么呢？\n其实，就算这一层本身不带检查选项了，它也会逐层向上检查，看它所继承的视图是否有检查选项；它继承的stu_v_2带检查选项，它进行逐层检查时发现条件不满足，于是就插入失败了。\n\n\n\n\nlocal\n本地。 \n比如，v2视图是基于v1视图的，如果在v2视图创建的时候指定了检查选项为 local ，但是v1视图创建时未指定检查选项。 则在执行检查时，知会检查v2，不会检查v2的关联视图v1，如下图：\n\n我们创建三个视图\ncreate or replace view stu_v_4 as select id,name from student where id &lt;=20;create or replace view stu_v_5 as select id,name from stu_v_4 where id &gt;=10 with cascaded check option ;create or replace view stu_v_6 as select id,name from stu_v_5 where id &lt;=15;\n\n\n对v5插入数据\n\ninsert into stu_v_5 values (6,&#x27;Tom&#x27;);insert into stu_v_5 values (13,&#x27;Tom&#x27;);insert into stu_v_5 values (21,&#x27;Tom&#x27;);\n\n\n只有6插入失败，因local不具备传递功能，你有检查选项就是有，没有就是没有，各自判断出来\n\n\n\n\n对v6插入数据\n\ninsert into stu_v_6 values (7,&#x27;Tom&#x27;);insert into stu_v_6 values (11,&#x27;Tom&#x27;);insert into stu_v_6 values (17,&#x27;Tom&#x27;);insert into stu_v_6 values (28,&#x27;Tom&#x27;);\n\n\n也只有7插入失败，原因同上\n\n\n\n\n更新要使视图可更新，视图中的行与基础表中的行之间必须存在一对一的关系。如果视图包含以下任何一 项，则该视图不可更新： \n\n聚合函数或窗口函数（SUM()、 MIN()、 MAX()、 COUNT()等）\nDISTINCT \nGROUP BY \nHAVING \nUNION 或者 UNION ALL\n\n我们来测试一下，创建一个带有count的视图：\ncreate view stu_v_count as select count(*) from student;\n\n\n\n上述的视图中，就只有一个单行单列的数据，如果我们对这个视图进行更新或插入的，将会报错。\ninsert into stu_v_count values(10);\n\n\n\n作用\n简单：视图不仅可以简化用户对数据的理解，也可以简化他们的操作。那些被经常使用的查询可以被定义为视图，从而使得用户不必为以后的操作每次指定全部的条件。 \n安全：数据库可以授权，但不能授权到数据库特定行和特定的列上。通过视图用户只能查询和修改他们所能见到的数据。\n**数据独立：**视图可帮助用户屏蔽真实表结构变化带来的影响。\n\n案例\n为了保证数据库表的安全性，开发人员在操作tb_user表时，只能看到的用户的基本字段，屏蔽 手机号和邮箱两个字段。\n\ncreate view tb_user_view as select id,name,profession,age,gender,status,createtime from tb_user;\n\n\n如下图，只有我们想让对方看见的字段，避免了敏感字段，比如密码什么的、\n\n\n\n\n查询每个学生所选修的课程（三张表联查），这个功能在很多的业务中都有使用到，为了简化操作，定义一个视图。\n\ncreate view tb_stu_course_view as select s.name student_name , s.no student_no ,c.name course_name from student s, student_course sc , course c where s.id = sc.studentid and sc.courseid = c.id;\n\n\n如下图，这样每次都只需要针对于这个视图进行操作即可，不用每次都写很长的连表条件。\n\n\n\n存储过程\n存储过程是事先经过编译并存储在数据库中的一段SQL语句的集合，调用存储过程可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。 \n\n存储过程思想上很简单，就是数据库 SQL 语言层面的代码封装与重用：\n\n\n特点\n封装，复用：可以把某一业务SQL封装在存储过程中，需要用到的时候直接调用即可。 \n**可以接收参数，也可以返回数据：**在存储过程中，可以传递参数，也可以接收返回值。 \n减少网络交互，效率提升： 如果涉及到多条SQL，每执行一次都是一次网络传 输。 而如果封装在存储过程中，我们只需要网络交互一次可能就可以了。\n\n基本语法创建CREATE PROCEDURE 存储过程名称 ([ 参数列表 ])BEGIN-- SQL语句END ;\n\n\n注意：在命令行中，执行创建存储过程的SQL时，需要通过关键字delimiter指定SQL语句的结束符（指定为除;的其他符号）。\n因为命令行自动识别;为结束符，它识别到begin和end中间的分号时，就认为这一条SQL语句已经写完了，就会出现错误。\n\n调用CALL 名称 ([ 参数 ]); \n\n查看-- 查询指定数据库的存储过程及状态信息SELECT * FROM INFORMATION_SCHEMA.ROUTINES WHERE ROUTINE_SCHEMA = &#x27;xxx&#x27;;  -- 查询某个存储过程的定义SHOW CREATE PROCEDURE 存储过程名称 ;  \n\n删除DROP PROCEDURE [IF EXISTS] 存储过程名称；\n\n变量在MySQL中变量分为三种类型: 系统变量、用户定义变量、局部变量。\n系统变量\n系统变量 是MySQL服务器提供，不是用户定义的，属于服务器层面。分为全局变量（GLOBAL）、会话 变量（SESSION）。\n\n\n查看系统变量\n\n-- 查看所有系统变量SHOW [SESSION | GLOBAL] VARIABLES ;  -- 可以通过LIKE模糊匹配方式查找变量SHOW [SESSION | GLOBAL] VARIABLES LIKE &#x27;......&#x27;;     --  查看指定变量的值SELECT @@[SESSION | GLOBAL]  系统变量名;                    \n\n查询所有系统变量：\n\n\n查看以auto开头的系统变量：\n\n\n知道名字直接查当前会话中的系统变量：\nselect @@session.autocommit;-- 全局同理\n\n\n\n\n设置系统变量\n\nSET [SESSION|GLOBAL] 系统变量名 = 值 ; SET @@[SESSION|GLOBAL] 系统变量名 = 值 ; \n\n注意:  如果没有指定SESSION&#x2F;GLOBAL，默认是SESSION，会话变量。   \n\n全局变量(GLOBAL): 全局变量针对于所有的会话。 \n会话变量(SESSION):  会话变量针对于单个会话，在另外一个会话窗口就不生效了。\n\n\nmysql服务重新启动之后，所设置的全局参数会失效，要想不失效，可以在 &#x2F;etc&#x2F;my.cnf 中配置。\n\n用户定义变量\n用户定义变量是用户根据需要自己定义的变量，用户变量不用提前声明，在用的时候直接用 “@变量名” 使用就可以。其作用域为当前连接，只能在当前会话中使用。\n\n\n赋值\n\nSET @var_name = expr [, @var_name = expr] ... ; SET @var_name := expr [, @var_name := expr] ... ; \n\n赋值时，可以使用 &#x3D; ，也可以使用 :&#x3D; 。\nSELECT @var_name := expr [, @var_name := expr] ... ; SELECT 字段名 INTO @var_name  FROM  表名;\n\n方式一：\nset @myname = &#x27;itcast&#x27;;set @myage :=18;set @mygender := &#x27;男&#x27;,@myhobby := &#x27;Java&#x27;;\n\n\n推荐使用:=，因为sql里面比较运算符只有=，用:=比较好区分。\n\n方式二\nselect @myphone :=&#x27;xiaomi&#x27;;select count(*) into @mycount from student;\n\n\n使用\n\nSELECT @var_name; \n\n如下展示刚刚生成的变量：\nselect @myname,@myage,@mygender,@myhobby;\n\n\n\nselect @myphone,@mycount;\n\n\n\n\n注意: 用户定义的变量无需对其进行声明或初始化，只不过获取到的值为NULL。\n\n局部变量\n局部变量是根据需要定义的在局部生效的变量，访问之前，需要DECLARE声明。可用作存储过程内的局部变量和输入参数，局部变量的范围是在其内声明的BEGIN … END块。\n\n\n声明\n\nDECLARE 变量名 变量类型 [DEFAULT ... ] ;\n\n变量类型就是数据库字段类型：INT、BIGINT、CHAR、VARCHAR、DATE、TIME等。\n\n赋值\n\nSET 变量名 = 值 ;SET 变量名 := 值 ;SELECT 字段名 INTO 变量名 FROM 表名 ... ; \n\n示例\n\ndefault后跟初始化值，如果不用初始值可以省略。\n\ncreate procedure p2()begin    declare stu_count int default 0;    set stu_count :=100;    select stu_count;end;call p2();\n\n\nif\n在if条件判断的结构中，ELSE IF 结构可以有多个，也可以没有。 ELSE结构可以有，也可以没有。\n\nIF  条件1  THEN     .....ELSEIF  条件2  THEN       -- 可选    .....ELSE                     -- 可选    .....END  IF;\n\n案例\n根据定义的分数score变量，判定当前分数对应的分数等级。 \n\nscore &gt;&#x3D; 85分，等级为优秀。 \nscore &gt;&#x3D; 60分 且 score &lt; 85分，等级为及格。 \nscore &lt; 60分，等级为不及格。\n\ncreate procedure p3()begin    declare score int default 58;    declare result varchar(10);    if score &gt;=85 then        set result :=&#x27;优秀&#x27;;    elseif score &gt;=60 then        set result :=&#x27;及格&#x27;;    else        set result :=&#x27;不及格&#x27;;    end if;    select result;end;call p3();\n\n上述的需求我们虽然已经实现了，但是也存在一些问题，比如：score 分数我们是在存储过程中定义死的，而且最终计算出来的分数等级，我们也仅仅是最终查询展示出来而已。 \n那么我们能不能，把score分数动态的传递进来，计算出来的分数等级是否可以作为返回值返回呢？答案是肯定的，我们可以通过接下来所讲解的参数来解决上述的问题。\n\n参数参数的类型，主要分为以下三种：IN、OUT、INOUT。 具体的含义如下：\n\n\n\n类型\n含义\n\n\n\nin\n该类参数作为输入，也就是需要调用时传入值\n\n\nout\n该类参数作为输出，也就是该参数可以作为返回值\n\n\ninout\n既可以作为输入参数，也可以作为输出参数\n\n\n语法：\nCREATE  PROCEDURE   存储过程名称 ([ IN/OUT/INOUT  参数名  参数类型 ])BEGIN    -- SQL语句END ;\n\n根据传入参数score，判定当前分数对应的分数等级，并返回。  \n\nscore &gt;&#x3D; 85分，等级为优秀。 \nscore &gt;&#x3D; 60分 且 score &lt; 85分，等级为及格。 \nscore &lt; 60分，等级为不及格。\n\ncreate procedure p4(in score int, out result varchar(10))begin    if score &gt;=85 then        set result :=&#x27;优秀&#x27;;    elseif score &gt;=60 then        set result :=&#x27;及格&#x27;;    else        set result :=&#x27;不及格&#x27;;    end if;end;call p4(70,@result);select @result;\n\n\ncasecase结构及作用，和我们在基础篇中所讲解的流程控制函数很类似。\n有两种语法格式：\n语法1：\n\n含义： 当case_value的值为 when_value1时，执行statement_list1，当值为 when_value2时执行statement_list2， 否则就执行 statement_list\n\nCASE  case_value    WHEN when_value1 THEN statement_list1   [ WHEN when_value2 THEN statement_list2] ...   [ ELSE statement_list ]END CASE;\n\n语法2：\n\n含义： 当条件search_condition1成立时，执行statement_list1，当条件search_condition2成 立时，执行statement_list2， 否则就执行 statement_list\n\nCASE  WHEN search_condition1 THEN statement_list1  [WHEN search_condition2 THEN statement_list2] ...  [ELSE statement_list]END CASE;\n\n案例\n根据传入的月份，判定月份所属的季节（要求采用case结构）。 \n\n1-3月份，为第一季度 \n4-6月份，为第二季度 \n7-9月份，为第三季度 \n10-12月份，为第四季度\n\ncreate procedure p5(in month int)begin    declare result varchar(10);    case        when month &gt;=1 and month &lt;=3 then            set result :=&#x27;第1季度&#x27;;        when month &gt;=4 and month &lt;=6 then            set result :=&#x27;第2季度&#x27;;        when month &gt;=7 and month &lt;=9 then            set result :=&#x27;第3季度&#x27;;        when month &gt;=10 and month &lt;=12 then            set result :=&#x27;第4季度&#x27;;        else            set result :=&#x27;非法参数&#x27;;    end case;    select concat(&#x27;你输入的月份为：&#x27;,month,&#x27;所属的季度为：&#x27;,result);end;\n\n\nwhilewhile 循环是有条件的循环控制语句。满足条件后，再执行循环体中的SQL语句。具体语法为：\n\n先判定条件，如果条件为true，则执行逻辑，否则，不执行逻辑\n\nWHILE 条件 DO    SQL逻辑...    END WHILE;\n\n案例\n计算从1累加到n的值，n为传入的参数值。\ncreate procedure p6(in n int)begin    declare total int default 0;    while n&gt;0 do        set total := total+n;        set n := n-1;    end while;    select total;end;\n\n\nrepeatrepeat是有条件的循环控制语句, 当满足until声明的条件的时候，则退出循环 。具体语法为：\n\n先执行一次逻辑，然后判定UNTIL条件是否满足，如果满足，则退出。如果不满足，则继续下一次循环，直到满足某个条件退出循环\n\nREPEAT    SQL逻辑...      UNTIL  条件END REPEAT;\n\n案例\n计算从1累加到n的值，n为传入的参数值。(使用repeat实现)\ncreate procedure p7(in n int)begin    declare total int default 0;    repeat        set total := total +n;        set n := n-1;    until n =0 end repeat;    select total;end;\n\n\nloop\nLOOP 实现简单的循环，如果不在SQL逻辑中增加退出循环的条件，可以用其来实现简单的死循环。 LOOP可以配合一下两个语句使用：\n\n\nLEAVE ：配合循环使用，退出循环。\nITERATE：必须用在循环中，作用是跳过当前循环剩下的语句，直接进入下一次循环。\n\n\n下面语法中出现的 begin_label，end_label，label 指的都是我们所自定义的标记。\n\n[begin_label:]  LOOP    SQL逻辑...  END  LOOP  [end_label];\n\nLEAVE  label;   -- 退出指定标记的循环体ITERATE  label; -- 直接进入下一次循环\n\n案例1\n计算从1累加到n的值，n为传入的参数值。\ncreate procedure p8(in n int)begin    declare total int default 0;    sum:loop        if n&lt;=0 then            leave sum;        end if;        set total :=total+n;        set n :=n-1;    end loop sum;    select total;end;\n\n案例2\n计算从1到n之间的偶数累加的值，n为传入的参数值。\ncreate procedure p9(in n int)begin    declare total int default 0;    sum:loop        if n&lt;=0 then            leave sum;        end if;        if n%2=1 then            set n :=n-1;            iterate sum;        end if;        set total :=total+n;        set n :=n-1;    end loop sum;    select total;end;\n\n\ncursor\n游标（CURSOR）是用来存储查询结果集的数据类型 , 在存储过程和函数中可以使用游标对结果集进行循环的处理。游标的使用包括游标的声明、OPEN、FETCH 和 CLOSE，其语法分别如下。\n\n\n声明游标\n\nDECLARE 游标名称 CURSOR FOR 查询语句 ;\n\n\n打开游标\n\nOPEN 游标名称 ;\n\n\n获取游标记录\n\nFETCH 游标名称 INTO 变量 [, 变量 ] ;\n\n\n关闭游标\n\nCLOSE 游标名称 ;\n\n案例\n根据传入的参数uage，来查询用户表tb_user中，所有的用户年龄小于等于uage的用户姓名 （name）和专业（profession），并将用户的姓名和专业插入到所创建的一张新表 (id,name,profession)中。\ncreate procedure p11(in uage int)begin    declare uname varchar(100);    declare upro varchar(100);    declare u_cursor cursor for select name,profession from tb_user where age &lt;= uage;    drop table if exists tb_user_pro;    create table if not exists tb_user_pro(         id int primary key auto_increment,         name varchar(100),         profession varchar(100)    );    open u_cursor;    while true do         fetch u_cursor into uname,upro;         insert into tb_user_pro values (null, uname, upro);    end while;    close u_cursor;end;\n\n但是当我们执行过后会报错\n\n找不到数据。\n\n\n\n我们在循环中用游标不断填写数据的时候，到最后一次它没有数据了，就会跳出循环，但是这时就会报错，那我们怎么解决呢？\n要想解决这个问题，就需要通过MySQL中提供的 条件处理程序 Handler 来解决\n\n条件处理程序\n条件处理程序（Handler）可以用来定义在流程控制结构执行过程中遇到问题时相应的处理步骤。具体语法为：\n\n-- for后面跟上的就是条件，满足什么样的条件我才执行handle这个动作-- statement：具体的SQL逻辑DECLARE handler_action HANDLER FOR condition_value [, condition_value]... statement; handler_action 的取值：      CONTINUE: 继续执行当前程序    EXIT: 终止执行当前程序condition_value 的取值：     SQLSTATE  sqlstate_value: 状态码，如 02000 -- 当我们执行SQL的时候，它抛出的SQL语句的状态码来决定到底执行continue还是exit    SQLWARNING: 所有以01开头的SQLSTATE代码的简写，SQL警告    NOT FOUND: 所有以02开头的SQLSTATE代码的简写，没有找到    SQLEXCEPTION: 所有没有被SQLWARNING 或 NOT FOUND 捕获的SQLSTATE代码的简写\n\n我们继续来完成在上一小节提出的这个需求，并解决其中的问题。\n\n通过SQLSTATE指定具体的状态码(02000)\n\ndeclare exit handler for SQLSTATE &#x27;02000&#x27; close u_cursor;\n\n\n这里的SQLSTATE的值其实就是上面报错的SQL状态码，表示只要出现这种情况，我们就会退出当前程序。\n\ncreate procedure p11(in uage int)begin    declare uname varchar(100);    declare upro varchar(100);    declare u_cursor cursor for select name,profession from tb_user where age &lt;= uage;    declare exit handler for SQLSTATE &#x27;02000&#x27; close u_cursor;  -- 也可以写作 not found    drop table if exists tb_user_pro;    create table if not exists tb_user_pro(         id int primary key auto_increment,         name varchar(100),         profession varchar(100)    );    open u_cursor;    while true do         fetch u_cursor into uname,upro;         insert into tb_user_pro values (null, uname, upro);    end while;    close u_cursor;end;\n\n这次不报错了：\n\n\n表中也有对应数据：\n\n\n\n具体的错误状态码，可以参考官方文档： \nhttps://dev.mysql.com/doc/refman/8.0/en/declare-handler.html  \nhttps://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html\n\n\n存储函数\n存储函数是有返回值的存储过程，存储函数的参数只能是IN类型的。\n\n具体语法如下：\nCREATE  FUNCTION   存储函数名称 ([ 参数列表 ])RETURNS  type  [characteristic ...]BEGIN\t-- SQL语句\tRETURN ...;END ;\n\ncharacteristic说明： \n\nDETERMINISTIC：相同的输入参数总是产生相同的结果。\nNO SQL ：不包含SQL语句。 \nREADS SQL DATA：包含读取数据的语句，但不包含写入数据的语句。\n\n案例\n计算从1累加到n的值，n为传入的参数值。\ncreate function fun1(n int)returns int deterministicbegin    declare total int default 0;        while n&gt;0 do        set total := total + n;        set n := n - 1;    end while;        return total;end;\n\n\nPS：其实用的很少，因为存储函数能解决的，存储过程肯定也可以。\n\n\n触发器\n触发器是与表有关的数据库对象，指在insert&#x2F;update&#x2F;delete之前(BEFORE)或之后(AFTER)，触发并执行触发器中定义的SQL语句集合。触发器的这种特性可以协助应用在数据库端确保数据的完整性  , 日志记录 , 数据校验等操作 。\n\n使用别名OLD和NEW来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。\n\n\n\n触发器类型\nnew和old\n\n\n\nINSERT 型触发器\nNEW表示将要或者已经新增的数据\n\n\nUPDATE 型触发器\nOLD表示修改之前的数据 ,NEW表示将要或已经修改后的数据\n\n\nDELETE 型触发器\nOLD表示将要或者已经删除的数据\n\n\n语法创建CREATE TRIGGER trigger_name BEFORE/AFTER  INSERT/UPDATE/DELETEON tbl_name FOR EACH ROW  -- 行级触发器BEGIN trigger_stmt ;END;\n\n查看SHOW TRIGGERS;\n\n删除-- 如果没有指定 schema_name，默认为当前数据库 。DROP TRIGGER [schema_name.]trigger_name ;  \n\n案例通过触发器记录 tb_user 表的数据变更日志，将变更日志插入到日志表user_logs中, 包含增加,  修改 , 删除 ;\n表结构准备:\ncreate table user_logs(  id int(11) not null auto_increment,  operation varchar(20) not null comment &#x27;操作类型,insert/update/delete&#x27;,  operate_time datetime not null comment &#x27;操作时间&#x27;,  operate_id int(11) not null comment &#x27;操作的ID&#x27;,  operate_params varchar(500) comment &#x27;操作参数&#x27;,  primary key(`id`))engine=innodb default charset=utf8;\n\n插入数据触发器create trigger tb_user_insert_trigger    after insert on tb_user for each rowbegin    insert into user_logs(id, operation, operate_time, operate_id, operate_params) values  (null, &#x27;insert&#x27;, now(), new.id,   concat(&#x27;插入的数据内容为: id=&#x27;,new.id,&#x27;,name=&#x27;,new.name, &#x27;, phone=&#x27;, NEW.phone, &#x27;, email=&#x27;, NEW.email, &#x27;, profession=&#x27;, NEW.profession));end;\n\n插入数据：\ninsert into tb_user(id, name, phone, email, profession, age, gender, status, createtime) VALUES (26,&#x27;三皇子&#x27;,&#x27;18809091212&#x27;,&#x27;erhuangzi@163.com&#x27;,&#x27;软件工程&#x27;,23,&#x27;1&#x27;,&#x27;1&#x27;,now());\n\n\n\n更新数据触发器create trigger tb_user_update_trigger    after update on tb_user for each rowbegin    insert into user_logs(id, operation, operate_time, operate_id, operate_params) values (null, &#x27;update&#x27;, now(), new.id, concat(&#x27;更新之前的数据: id=&#x27;,old.id,&#x27;,name=&#x27;,old.name, &#x27;, phone=&#x27;, old.phone, &#x27;, email=&#x27;, old.email, &#x27;, profession=&#x27;, old.profession,&#x27; | 更新之后的数据: id=&#x27;,new.id,&#x27;,name=&#x27;,new.name, &#x27;, phone=&#x27;, NEW.phone, &#x27;, email=&#x27;, NEW.email, &#x27;, profession=&#x27;, NEW.profession));end;\n\n更新数据：\nupdate tb_user set profession = &#x27;会计&#x27; where id &lt;= 5;\n\n\n可以看见插入了五条日志，这是因为MySQL只支持行级触发器，这里更新五条数据就是五条日志了。\n\n\n\n删除数据触发器create trigger tb_user_delete_trigger    after delete on tb_user for each rowbegin    insert into user_logs(id, operation, operate_time, operate_id, operate_params) values  (null, &#x27;delete&#x27;, now(), old.id, concat(&#x27;删除之前的数据: id=&#x27;,old.id,&#x27;,name=&#x27;,old.name, &#x27;, phone=&#x27;,old.phone, &#x27;, email=&#x27;, old.email, &#x27;, profession=&#x27;, old.profession));end\n\n删除数据：\ndelete from tb_user where id = 26;\n\n\n\n\n\n\n\n\n\n","categories":["MySQL"],"tags":["MySQL"]},{"title":"RabbitMQ","url":"/2025/07/26/RabbitMQ/","content":"基本概念消息Broker，目前常见的实现方案就是消息队列（MessageQueue），简称为MQ。\nRabbitMQ是基于Erlang语言开发的开源消息通信中间件，官网地址：Messaging that just works — RabbitMQ\n安装过程网上有很多，在此我就省略了。\n\n\n\n其中包含几个概念：\n\npublisher：生产者，也就是发送消息的一方\nconsumer：消费者，也就是消费消息的一方\nqueue：队列，存储消息。生产者投递的消息会暂存在消息队列中，等待消费者处理\nexchange：交换机，负责消息路由。生产者发送的消息由交换机决定投递到哪个队列。\nvirtual host：虚拟主机，起到数据隔离的作用。每个虚拟主机相互独立，有各自的exchange、queue\n\n整个过程梳理下来如下：\n生产者[发送消息给交换机，交换机路由消息（并不是存储），然后决定将消息投递到某个或者某些队列]，这时，队列存储这些消息，等待消费者处理，消费者根据自身的需求和能力选择处理消息。\n\n注意：中括号内并不是必须的操作。\n\n在同一个mq集群下，可以有多个virtual host，他们之间互相隔离，有各自的exchange、queue。\n\n发送消息上面我们了解了基本过程，我们就来演示一下。\n\n创建两个queue\n\n\n\n如下：\n\n\n\n操作交换机发出消息\n\n我们将amq.fanout绑定我们刚刚创建的两个队列，并发送消息Hello,mq!。\n\n\n消息已发送！\n\n\n\n成功接收消息\n\n\n\n也可以看见消息的具体内容，也可以发现这里的消息无法更改\n\n\n\n数据隔离上面所说同一个mq集群下，可以有多个virtual host，他们之间互相隔离，有各自的exchange、queue。\n我们也可以进行验证。\n\n创建一个T00超级管理员\n\n\n\n但是这个时候，我们无法操作，队列，交换机等，因为“没有我们名下”的虚拟主机。\n\n创建虚拟主机\n\n切换成我们新创建的用户，然后创建一个虚拟主机\n\n\n他们两个虚拟主机的数据是相互隔离的\n\n\n\nSpringAMQP将来我们开发业务功能的时候，肯定不会在控制台收发消息，而是应该基于编程的方式。由于RabbitMQ采用了AMQP协议，因此它具备跨语言的特性。任何语言只要遵循AMQP协议收发消息，都可以与RabbitMQ交互。并且RabbitMQ官方也提供了各种不同语言的客户端。\n\n\n查看官方文档，java操作MQ的hello，world都及其复杂\n\n\n\n\nSpring的官方刚好基于RabbitMQ提供了这样一套消息收发的模板工具：SpringAMQP。并且还基于SpringBoot对其实现了自动装配，使用起来非常方便。\n入门案例\n引入相关依赖\n\n使用SpringAMQP，我们要先引入相关依赖（这里是在父工程中引入的）\n&lt;!--AMQP依赖，包含RabbitMQ--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;\n\n我们以它为父工程创建两个微服务模块来模拟消息的收发过程。\n\n\n\n配置RabbitMQ服务端信息\n\n为了方便测试，我们先新建一个simple.queue\n先配置MQ地址，在publisher服务的application.yml中添加配置\nspring:  rabbitmq:    host: 192.168.100.128 # 你的虚拟机IP    port: 5672 # 端口    virtual-host: /T00 # 虚拟主机    username: TOO # 用户名    password: 101 # 密码\n\n\n发送消息\n\nspringAMQP中封装了一个RabbitTemplate工具类，我们可以直接使用工具类来发送消息。\n@Autowiredprivate RabbitTemplate rabbitTemplate;@Testvoid testSendMessage2Queue()&#123;    String queueName = &quot;simple.queue&quot;;    String msg = &quot;helloe,mq!&quot;;    rabbitTemplate.convertAndSend(queueName,msg);&#125;\n\n直接运行，可以在控制台中找到发送的消息\n\n\n\n接收消息\n\nspringAMQP中封装了一个注解，直接在注解中queues数组中指定需要监听的队列名字即可\n@RabbitListener(queues = &quot;simple.queue&quot;)public void listenerSimpleQueue(String msg)&#123;        System.out.println(&quot;收到了消息：&quot;+ msg);    &#125;\n\n运行之后成功收到了消息\n\n\n\n这里被注解的方法的参数类型应和生产者的一致，@RabbitListener注解的作用就是将接收到的消息封装在方法的参数中。\n\n\nWorkQueues模型Work queues，任务模型。简单来说就是让多个消费者绑定到一个队列，共同消费队列中的消息，每个消息只会被一个消费者处理一次，用来实现任务的“分工处理”。\n\n\n当消息处理比较耗时的时候，可能生产消息的速度会远远大于消息的消费速度。长此以往，消息就会堆积越来越多，无法及时处理。\n此时就可以使用work 模型，多个消费者共同处理消息处理，消息处理的速度就能大大提高了。\n接下来，我们就来模拟这样的场景。\n首先，我们在控制台创建一个新的队列，命名为work.queue：\n\nRabbitMQ 有两种分发模式：\n\n默认轮询发配\n\n公平发配\n\n\n\n\n我们首先模拟轮询发配\n轮询发配两个消费者性能相同\n消息发送\n\n@Testvoid testWorkQueue() throws InterruptedException &#123;    String queueName = &quot;work.queue&quot;;    for (int i = 1; i &lt;= 50; i++) &#123;        String msg = &quot;hello,worker!,message_&quot; + i;        rabbitTemplate.convertAndSend(queueName,msg);        Thread.sleep(20);    &#125;&#125;\n\n\n消息接收\n\n要模拟多个消费者绑定同一个队列，我们在consumer服务的SpringRabbitListener中添加2个新的方法：\n@RabbitListener(queues = &quot;work.queue&quot;)public void listenerWorkQueue1(String msg)&#123;    System.out.println(&quot;消费者1 收到了消息：&quot; + msg);&#125;@RabbitListener(queues = &quot;work.queue&quot;)public void listenerWorkQueue2(String msg)&#123;    System.err.println(&quot;消费者1 收到了消息：&quot; + msg);&#125;\n\n\n运行\n\n\n\n可以看见，两个消费者如果性能一致时类似于轮询，一人消费一条消息，将消息均分了。\n两个消费者性能不同让两个消费者接收消息后停顿不同时间来实现模拟性能不同，消费者1每秒可以处理50条消息，而消费者2只能处理5条。\n@RabbitListener(queues = &quot;work.queue&quot;)public void listenerWorkQueue1(String msg) throws InterruptedException &#123;    System.out.println(&quot;消费者1 收到了消息：&quot; + msg);    Thread.sleep(20);&#125;@RabbitListener(queues = &quot;work.queue&quot;)public void listenerWorkQueue2(String msg) throws InterruptedException &#123;    System.err.println(&quot;消费者2 收到了消息：&quot; + msg);    Thread.sleep(200);&#125;\n\n结果如下：\n\n\n\n\n可以看见他们还是均分了50条消息，后面的时间全是性能比较差的消费者2在处理，性能好的消费者1反倒空闲了。\n这样显然是不合理的，浪费了系统的性能，这时我们就应该使用公平分配模式。\n公平分配公平分配通俗来讲，就是能者多劳。\n在spring中有一个简单的配置，可以解决这个问题。我们修改consumer服务的application.yml文件，添加配置：\nspring:  rabbitmq:    listener:      simple:        prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息\n\n这样配置之后再次运行：\n\n\n可以发现，由于消费者1处理速度较快，所以处理了更多的消息；消费者2处理速度较慢，只处理了6条消息。而最终总的执行耗时也在1秒左右，大大提升。\n这样充分利用了每一个消费者的处理能力，可以有效避免消息积压问题。\n\n交换机Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！\n交换机的类型有四种：\n\nFanout：广播，将消息交给所有绑定到交换机的队列。我们最早在控制台使用的正是Fanout交换机。\nDirect：订阅，基于RoutingKey（路由key）发送给订阅了消息的队列\nTopic：通配符订阅，与Direct类似，只不过RoutingKey可以使用通配符\nHeaders：头匹配，基于MQ的消息头匹配，用的较少。\n\n接下来，我们就来一一验证。\nFanout\n准备工作\n\n\n创建一个名为 T00.fanout的交换机，类型是Fanout\n创建两个队列fanout.queue1和fanout.queue2，绑定到交换机T00.fanout\n\n\n\n过程和上面控制台发送消息一样，就略过了。\n\n消息发送\n\nfanout交换机没有这个key，传入一个空字符串即可：\n\n\n@Testpublic void testFanoutExchange() &#123;    // 交换机名称    String exchangeName = &quot;T00.fanout&quot;;    // 消息    String message = &quot;hello, everyone!&quot;;    rabbitTemplate.convertAndSend(exchangeName, &quot;&quot;, message);&#125;\n\n\n消息接收\n\n@RabbitListener(queues = &quot;fanout.queue1&quot;)public void listenerFanoutQueue1(String msg)&#123;    System.out.println(&quot;消费者1 收到了fanout.queue1消息：&quot; + msg);&#125;@RabbitListener(queues = &quot;fanout.queue2&quot;)public void listenerFanoutQueue2(String msg)&#123;    System.out.println(&quot;消费者2 收到了fanout.queue2消息：&quot; + msg);&#125;\n\n\n运行\n\n两者均接收到了消息，就如同广播一样，在广播的范围内，每个人都能收到消息。\n\n\n\nDirect在Fanout交换机中，一条消息，会被所有订阅的队列都消费。但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。\nDirect交换机有以下特点：\n\n队列与交换机的绑定：不能是任意绑定了，而是要指定一个RoutingKey（路由key）\n\n消息的发送方在向 Exchange发送消息时，也必须指定消息的 RoutingKey\n\nExchange会根据绑定队列的Routingkey和所路由消息的Routingkey是否匹配，进行消息的发送，也就是是说只有队列的Routingkey与消息的 Routing key完全一致，才会接收到消息。\n\n\n\n接下来我们进行演示：\n\n准备工作\n\n\n创建一个名为T00.direct的交换机，类型是direct\n再分别创建一个Routing key为“red”，“blue”的direct.queue1和一个Routing key为“red”，“yellow”的direct.queue2\n\n\n\n\n消息发送\n\n@Testpublic void testSendDirectExchange() &#123;    // 交换机名称    String exchangeName = &quot;T00.direct&quot;;    // 消息    String message = &quot;红色&quot;;    rabbitTemplate.convertAndSend(exchangeName, &quot;red&quot;, message);&#125;\n\n@Testpublic void testSendDirectExchange() &#123;    // 交换机名称    String exchangeName = &quot;T00.direct&quot;;    // 消息    String message = &quot;蓝色&quot;;    rabbitTemplate.convertAndSend(exchangeName, &quot;bule&quot;, message);&#125;\n\n\n消息接收\n\n@RabbitListener(queues = &quot;direct.queue1&quot;)public void listenerDirectQueue1(String msg)&#123;    System.out.println(&quot;消费者1 收到了direct.queue1消息：&quot; + msg);&#125;@RabbitListener(queues = &quot;direct.queue2&quot;)public void listenerDirectQueue2(String msg)&#123;    System.out.println(&quot;消费者2 收到了direct.queue2消息：&quot; + msg);&#125;\n\n\n运行\n\n如果是red，他们都可以收到\n\n\n但是如果是blue，就只有2才能收到\n\n\n\nTopic其实Topic与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列，唯一的区别就是Topic可以让队列在绑定BindingKey 的时候使用通配符。\nBindingKey 一般都是有一个或多个单词组成，多个单词之间以.分割，例如： item.insert\n通配符规则：\n\n#：匹配一个或多个词\n*：只能匹配1个词\n\n举例：\n\nitem.#：能够匹配item.spu.insert 或者 item.spu\nitem.*：只能匹配item.spu\n\n现在我们来实现一下：\n\n\n准备工作\n\n假如此时publisher发送的消息使用的RoutingKey共有四种：\n\nchina.news 代表有中国的新闻消息；\nchina.weather 代表中国的天气消息；\njapan.news 则代表日本新闻\njapan.weather 代表日本的天气消息；\n\n解释：\n\ntopic.queue1绑定的是china.# ，凡是以 china.开头的routing key 都会被匹配到，包括：\nchina.news\nchina.weather\n\n\ntopic.queue2绑定的是#.news ，凡是以 .news结尾的 routing key 都会被匹配。包括:\nchina.news\njapan.news\n\n\n\n\n\n\n发送消息\n\n@Testpublic void testSendTopicExchange() &#123;    // 交换机名称    String exchangeName = &quot;T00.topic&quot;;    // 消息    String message = &quot;*****&quot;;    rabbitTemplate.convertAndSend(exchangeName, &quot;***&quot;, message);&#125;\n\n\n接收消息\n\n@RabbitListener(queues = &quot;topic.queue1&quot;)public void listenerTopicQueue1(String msg)&#123;    System.out.println(&quot;消费者1 收到了topic.queue1消息：&quot; + msg);&#125;@RabbitListener(queues = &quot;topic.queue2&quot;)public void listenerTopicQueue2(String msg)&#123;    System.out.println(&quot;消费者2 收到了topic.queue2消息：&quot; + msg);&#125;\n\n\n运行\n\nroutingKey为japan.news时，只有消费者2收到了消息\n\n\nroutingKey为china.news时，都收到了消息\n\n\nroutingKey为china.wether时，只有消费者1收到了消息\n\n\n符合我们的推测。\n\n声明队列和交换机在之前我们都是基于RabbitMQ控制台来创建队列、交换机。但是在实际开发时，队列和交换机是程序员定义的，将来项目上线，又要交给运维去创建。那么程序员就需要把程序中运行的所有队列和交换机都写下来，交给运维。在这个过程中是很容易出现错误的。\n因此推荐的做法是由程序启动时检查队列和交换机是否存在，如果不存在自动创建。\n基本APISpringAMQP提供了一个Queue类，用来创建队列：\n\n\nSpringAMQP还提供了一个Exchange接口，来表示所有不同类型的交换机：\n\n\n我们可以自己创建队列和交换机，不过SpringAMQP还提供了ExchangeBuilder来简化这个过程：\n\n\n而在绑定队列和交换机时，则需要使用BindingBuilder类来创建Binding对象：\n\n\n\n一般这些的声明都在消费者中\n\n\n我们有两种方法声明队列和交换机\n\nnew一个对应的类，在后面指出名字即可\n\n调用builder函数\n\n\n下面我们共同使用两种办法来声明交换机和队列以及他们的绑定关系。\nfanout@Configurationpublic class FanoutConfiguration &#123;    /**     * 声明交换机     * @return Fanout类型交换机     */    @Bean    public FanoutExchange fanoutExchange()&#123;        // ExchangeBuilder.fanoutExchange(&quot;TOO.fanout2&quot;).build();        return new FanoutExchange(&quot;TOO.fanout2&quot;);  //默认持久，至于什么是持久，下面一篇文章会进行讲解    &#125;    /**     * 第1个队列     */    @Bean    public Queue fanoutQueue3()&#123;        return new Queue(&quot;fanout.queue3&quot;);  //默认持久    &#125;    /**     * 绑定队列和交换机     */    @Bean    public Binding bindingQueue1(Queue fanoutQueue3, FanoutExchange fanoutExchange)&#123;        return BindingBuilder.bind(fanoutQueue3).to(fanoutExchange);    &#125;    /**     * 第2个队列     */    @Bean    public Queue fanoutQueue4()&#123;        return new Queue(&quot;fanout.queue4&quot;);    &#125;    /**     * 绑定队列和交换机     */    @Bean    public Binding bindingQueue2(Queue fanoutQueue4, FanoutExchange fanoutExchange)&#123;        return BindingBuilder.bind(fanoutQueue4).to(fanoutExchange);    &#125;&#125;\n\n\n运行之后，交换机、队列、绑定关系都成功声明了。\n\n\n\n\n\n\n\n\ndirect对于一个direct的交换机进行绑定操作会显得比较繁琐（因为builder函数一次只能设置一个key）\n@Configurationpublic class DirectConfiguration &#123;    /**     * 声明交换机     * @return Direct类型交换机     */    @Bean    public DirectExchange directExchange()&#123;        return ExchangeBuilder.directExchange(&quot;TOO.direct&quot;).build();    &#125;    /**     * 第1个队列     */    @Bean    public Queue directQueue1()&#123;        return new Queue(&quot;direct.queue1&quot;);    &#125;    /**     * 绑定队列和交换机     */    @Bean    public Binding bindingQueue1WithRed(Queue directQueue1, DirectExchange directExchange)&#123;        return BindingBuilder.bind(directQueue1).to(directExchange).with(&quot;red&quot;);    &#125;    /**     * 绑定队列和交换机     */    @Bean    public Binding bindingQueue1WithBlue(Queue directQueue1, DirectExchange directExchange)&#123;        return BindingBuilder.bind(directQueue1).to(directExchange).with(&quot;blue&quot;);    &#125;    /**     * 第2个队列     */    @Bean    public Queue directQueue2()&#123;        return new Queue(&quot;direct.queue2&quot;);    &#125;    /**     * 绑定队列和交换机     */    @Bean    public Binding bindingQueue2WithRed(Queue directQueue2, DirectExchange directExchange)&#123;        return BindingBuilder.bind(directQueue2).to(directExchange).with(&quot;red&quot;);    &#125;    /**     * 绑定队列和交换机     */    @Bean    public Binding bindingQueue2WithYellow(Queue directQueue2, DirectExchange directExchange)&#123;        return BindingBuilder.bind(directQueue2).to(directExchange).with(&quot;yellow&quot;);    &#125;&#125;\n\nholy shit，这种声明方式好麻烦，因此伟大的springAMQP中还提供了一种注解声明队列和交换机的方法。\n\n注解声明可以在消费者注解中用bindings参数来声明队列，交换机和绑定关系，具体参数如下：\n\n\n基本框架如下：\n@RabbitListener(bindings = @QueueBinding(        value = @Queue(),        exchange = @Exchange(),        key = &#123;&#125;))\n\n我们现在删除以前于direct相关的交换机和队列，来试试用注解声明：\n@RabbitListener(bindings = @QueueBinding(        value = @Queue(name = &quot;direct.queue1&quot;,durable = &quot;true&quot;),        exchange = @Exchange(name = &quot;T00.direct&quot;,type = ExchangeTypes.DIRECT),        key = &#123;&quot;red&quot; , &quot;blue&quot;&#125;))public void listenerDirectQueue1(String msg)&#123;    System.out.println(&quot;消费者1 收到了direct.queue1消息：&quot; + msg);&#125;\n\n@RabbitListener(bindings = @QueueBinding(        value = @Queue(name = &quot;direct.queue2&quot;,durable = &quot;true&quot;),        exchange = @Exchange(name = &quot;T00.direct&quot;,type = ExchangeTypes.DIRECT),        key = &#123;&quot;red&quot; , &quot;yellow&quot;&#125;))public void listenerDirectQueue2(String msg)&#123;    System.out.println(&quot;消费者2 收到了direct.queue2消息：&quot; + msg);&#125;\n\n运行之后可以看见成功的创建了。\n\n\n\n消息转换器前面我们说到生产者发送消息的类型最好与消费者参数的类型相同，但是那是接收消息，那发送消息是否有什么限定呢？\n默认消息转换器我们先来试试它默认的消息转换器\n\n创建一个object.queue队列用于接收\n编写一个testSendObject()用于发送消息（json格式）\n\n@Testpublic void testSendObject() &#123;    Map&lt;String,Object&gt; msg = new HashMap&lt;&gt;();    msg.put(&quot;name&quot;,&quot;jack&quot;);    msg.put(&quot;age&quot;,18);    rabbitTemplate.convertAndSend(&quot;object.queue&quot;,msg);&#125;\n\n在发送消息过后，我们来到mq的控制台查看接收到的消息：\n\n\n它并不是json格式的信息，反而是一堆看不懂的东西，这是为什么呢？\n因为它默认的消息转换器是将消息jdk序列化，jdk序列化存在安全漏洞，可读性比较差（如果以后发送的消息是一个体积很大，很复杂的json，jdk序列化的消息很难维护）的缺点，那我们有没有什么方法可以解决呢？\n答案就是我们自己配置消息转换器来替换默认的转换器。\n配置JSON转换器\n在publisher和consumer两个服务中都引入依赖（父工程中引入即可）：\n\n&lt;dependency&gt;    &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt;    &lt;artifactId&gt;jackson-dataformat-xml&lt;/artifactId&gt;    &lt;version&gt;2.9.10&lt;/version&gt;&lt;/dependency&gt;\n\n注意，如果项目中引入了spring-boot-starter-web依赖，则无需再次引入Jackson依赖。\n\n配置消息转换器：\n\n在publisher和consumer两个服务的启动类中添加一个Bean即可：\n@Beanpublic MessageConverter jacksonmessageConverter()&#123;    return new Jackson2JsonMessageConverter();&#125;\n\n再次发送，成功转换了\n\n\n\n\n\n\n\n\n\n\n","categories":["MQ"],"tags":["MQ"]},{"title":"消息的可靠性","url":"/2025/07/27/%E6%B6%88%E6%81%AF%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7/","content":"消息丢失消息从发送者发送消息，到消费者处理消息，需要经过的流程是这样的：\n\n\n消息从生产者到消费者的每一步都可能导致消息丢失：\n\n发送消息时消息丢失\n生产者发送消息到交换机时\n交换机把消息发送给队列时\n\n\nMQ导致消息丢失\n消息没来得及发送给消费者就消失了\n\n\n消费者拿到消息后未消费导致消息丢失\n消费者拿到消息没来得及消费就消失了\n\n\n\n消息的可靠性综上，我们要解决消息丢失问题，保证MQ的可靠性，就必须从3个方面入手：\n\n确保生产者一定把消息发送到MQ\n确保MQ不会将消息弄丢\n确保消费者一定要处理消息\n\n生产者的可靠性生产者确认生产者在向交换机传输消息时，有可能消息根本无法到达交换机，或者到达交换机后没有成功的将消息传递给队列，所以MQ针对这两点分别做出了保障，我们称之为生产者的确认。\nConfirm模式：确保消息送达交换机当消息成功到达 RabbitMQ 的交换机（Exchange）时，RabbitMQ 就会返回 ack 投递失败返回nack。\n想要开启confirm模式也很简单，在publisher模块的application.yaml中添加配置：\nspring:  rabbitmq:    publisher-confirm-type: correlated # 开启publisher confirm机制，并设置confirm类型\n\n这里publisher-confirm-type有三种模式可选：\n\nnone：关闭confirm机制\nsimple：同步阻塞等待MQ的回执（等信息返回了再执行其他操作）\ncorrelated：MQ异步回调返回回执（发完消息直接做其他事情，有消息回调回来再进行处理）\n\nReturn回退机制：确保消息路由成功在消息到达Exchange过后，MQ会返回ack，但是这并不代表消息就可以成功发送给队列，因此还需要一层return机制来保证：确保消息成功从交换机路由到目标队列，未路由的消息能被回退感知。\n想要开启confirm模式也很简单，在publisher模块的application.yaml中添加配置：\nspring:  rabbitmq:    publisher-confirm-type: correlated  # 启用 confirm 机制    publisher-returns: true            # 启用 return 回调（还需 mandatory 才生效）    template:      mandatory: true                  # 配合 return 生效，消息未路由才触发回调，默认false会在失败后直接丢弃消息\n\n生产者确认机制的实现经过梳理整个生产者确认机制如下图：\n\n\n我们现在来实现一下：\n\n配置publisher模块的application.yaml\n\nspring:  rabbitmq:    publisher-confirm-type: correlated  # 启用 confirm 机制    publisher-returns: true            # 启用 return 回调（还需 mandatory 才生效）    template:      mandatory: true                  # 配合 return 生效，消息未路由才触发回调\n\n\n配置ConfirmCallback\n\n由于每个消息发送时的处理逻辑不一定相同，因此ConfirmCallback需要在每次发消息时定义。具体来说，是在调用RabbitTemplate中的convertAndSend方法时，多传递一个参数：\n\n\n这里的CorrelationData中包含两个核心的东西：\n\nid：消息的唯一标示，MQ对不同的消息的回执以此做判断，避免混淆\nSettableListenableFuture：回执结果的Future对象\n\n将来MQ的回执就会通过这个Future来返回，我们可以提前给CorrelationData中的Future添加回调函数来处理消息回执：\n\n\n\n配置ReturnCallback\n\n每个RabbitTemplate只能配置一个ReturnCallback，因此我们可以在配置类中统一设置，是一种初始化的操作，我们可以使用@PostConstruct来实现。\n（@PostConstruct 是 Java 中的一个注解，常用于标记方法，使其在对象实例化并完成依赖注入后立即执行。它通常用于初始化逻辑，例如加载配置、建立连接等。）\n我们建立一个MqReturnConfig配置类：\n@Slf4j@AllArgsConstructor@Configurationpublic class MqReturnConfig &#123;    private final RabbitTemplate rabbitTemplate;    @PostConstruct    public void init()&#123;        rabbitTemplate.setReturnsCallback(new RabbitTemplate.ReturnsCallback() &#123;            @Override            public void returnedMessage(ReturnedMessage returned) &#123;                log.debug(&quot;exchange: &#123;&#125;&quot;, returned.getExchange());                log.debug(&quot;routingKey: &#123;&#125;&quot;, returned.getRoutingKey());                log.debug(&quot;message: &#123;&#125;&quot;, returned.getMessage());                log.debug(&quot;replyCode: &#123;&#125;&quot;, returned.getReplyCode());                log.debug(&quot;replyText: &#123;&#125;&quot;, returned.getReplyText());            &#125;        &#125;);    &#125;&#125;\n\n\n发送消息及配置confirm\n\n@Testvoid testConfigCallback() throws InterruptedException &#123;    //1.创建cd    CorrelationData cd = new CorrelationData(UUID.randomUUID().toString());    //2.添加confirm callback    cd.getFuture().addCallback(new ListenableFutureCallback&lt;CorrelationData.Confirm&gt;() &#123;        @Override        public void onSuccess(CorrelationData.Confirm result) &#123;            log.debug(&quot;收到confirm callback回执&quot;);            if(result.isAck())&#123;                // 消息发送成功                log.debug(&quot;消息发送成功，收到ack&quot;);            &#125;else &#123;                log.error(&quot;消息发送失败，收到nack,原因：&#123;&#125;&quot; ,result.getReason());            &#125;        &#125;        @Override        public void onFailure(Throwable ex) &#123;            log.error(&quot;消息回调失败&quot;,ex);        &#125;    &#125;);   rabbitTemplate.convertAndSend(&quot;T00.direct&quot;,&quot;red&quot;,&quot;hello&quot;,cd);    Thread.sleep(2000);  //因为需要等待消息发出才能得到回调&#125;\n\n\n运行\n\n所有信息均正确时，返回了ack：\n\n\n\n\n当路由key不正确时，也返回了ack并且返回了我们编写的ReturnCallback配置类（到达交换机，但没有成功路由）：\n\n\n\n\n当无法连接到时，也就是没有正常到达交换机时，返回了nack，同时给出了原因：\n\n\n\n\n连接可靠性：生产者自动重连机制我们不难想到生产者发送消息时，可能会出现网络故障，导致与MQ的连接中断，这样也会导致消息的丢失。\n为了解决这个问题，SpringAMQP提供的消息发送时的重试机制。即：当RabbitTemplate与MQ连接超时后，多次重试。\n修改publisher模块的application.yaml文件，添加下面的内容：\nspring:  rabbitmq:    connection-timeout: 1s # 设置MQ的连接超时时间    template:      retry:        enabled: true # 开启超时重试机制        initial-interval: 1000ms # 失败后的初始等待时间        multiplier: 1 # 失败后下次的等待时长倍数，下次等待时长 = initial-interval * multiplier        max-attempts: 3 # 最大重试次数\n\n也可以看见它的默认值和上面是一样的。\n\n\n我们利用命令停掉RabbitMQ服务：\ndocker stop mq\n\n然后测试发送一条消息，会发现会每隔1秒重试1次，（发送1s发现发送失败，再间隔1s重试，所以两次发送时间间隔2s）总共重试了3次。消息发送的超时重试机制配置成功了！\n\n\n注意：\n\n这个重试是连接时的重试，如果是在消息发送抛出异常时，他是不会充实的。\n当网络不稳定的时候，利用重试机制可以有效提高消息发送的成功率。不过SpringAMQP提供的重试机制是阻塞式的重试，也就是说多次重试等待的过程中，当前线程是被阻塞的。\n如果对于业务性能有要求，建议禁用重试机制。如果一定要使用，请合理配置等待时长和重试次数，当然也可以考虑使用异步线程来执行发送消息的代码。\n\n\n总结为了保证消息在发送的过程中是稳定的，我们采用了生产者确认的方式，\n所谓生产者确认就是使用confirm机制和return回退机制保证消息可以稳定到达队列当中；\nconfirm机制可以通过修改配置文件来生效，它会在消息成功投递到交换机之后返回ack来告知我们它是否到达交换机，如果没有则返回nack；\nreturn回退机制是发生在消息成功投递到交换机之后，这时也会返回ack，但是他将消息路由到队列的过程中可能会失败，这时我们如果还配置了mandatory为ture，MQ就会将消息回调给生产者，否则会直接丢弃这条消息；\n最后我们还可以通过配置confirmcallback和returncallback来设置我们在不同情况下该做什么。\n\n消息持久化为了保证消息在传输的过程中是稳定的，我们需要配置消息持久化，所谓持久化其实就是将MQ存储在内存中的消息写到磁盘上面。\n内存空间有限，当消费者故障或者处理过慢时，会导致消息积压，引发MQ阻塞（mq内存满了之后，会做一个PageOut操作将一些较老的消息转移到磁盘上，这个期间，消息时阻塞的，不能执行其他操作）我们来验证一下，将一条消息发100w次：\n@Testvoid testPageOut()&#123;    Message msg = MessageBuilder            .withBody(&quot;101&quot;.getBytes(StandardCharsets.UTF_8))            .setDeliveryMode(MessageDeliveryMode.NON_PERSISTENT)            .build();    for (int i = 0; i &lt; 1000000; i++) &#123;        rabbitTemplate.convertAndSend(&quot;simple.queue&quot;,msg);    &#125;\n\n可以看见速率出现了明显的低谷，pageout的消息数量也巨大\n\n\n所以实现数据持久化是很有必要的，包括：\n\n交换机持久化\n队列持久化\n消息持久化\n\n实现队列和交换机持久化，都是将Durable设置为true即可；\n消息持久化是在发送消息时附带值为Persistent的properties：\n\n\n\n持久化的消息在mq重启后，消息依旧在\n\n与上面非持久化的消息相比，没有了太大的速率下跌，没有pageout，但是似乎还是不太理想。\n\n\nLazyQueue从RabbitMQ的3.6.0版本开始，就增加了Lazy Queues的模式，也就是惰性队列。惰性队列的特征如下：\n\n接收到消息后直接存入磁盘而非内存\n消费者要消费消息时才会从磁盘中读取并加载到内存（也就是懒加载）\n支持数百万条的消息存储\n\n而在3.12版本之后，LazyQueue已经成为所有队列的默认格式。因此官方推荐升级MQ为3.12版本或者所有队列都设置为LazyQueue模式。\n控制台配置代码配置\n添加x-queue-mod=lazy参数也可设置队列为Lazy模式：\n\n@Beanpublic Queue lazyQueue()&#123;    return QueueBuilder            .durable(&quot;lazy.queue&quot;)            .lazy() // 开启Lazy模式            .build();&#125;\n\n\n当然，我们也可以基于注解来声明队列并设置为Lazy模式：\n\n@RabbitListener(queuesToDeclare = @Queue(        name = &quot;lazy.queue&quot;,        durable = &quot;true&quot;,        arguments = @Argument(name = &quot;x-queue-mode&quot;, value = &quot;lazy&quot;)))public void listenLazyQueue(String msg)&#123;    ....&#125;\n\n测试一遍发现速率 基本一直处于峰值，而且没有在内存中（没有消费者接收这个消息）。\n\n\n总结为了保证消息的持久化，我们要将队列，交换机均设置为持久化的，发出的消息也要是持久化消息，这样就算mq挂了，消息被持久化到了硬盘上，也不会消失。\n\n消费者的可靠性同生产者一样，消费者也需要有确认机制来保证消息能被正常消费。\n消费者确认为了确认消费者是否成功处理消息，RabbitMQ提供了消费者确认机制。即：当消费者处理消息结束后，应该向RabbitMQ发送一个回执，告知RabbitMQ自己消息处理状态。回执有三种可选值：\n\nack：成功处理消息，RabbitMQ从队列中删除该消息\nnack：消息处理失败，RabbitMQ需要再次投递消息\nreject：消息处理失败并拒绝该消息，RabbitMQ从队列中删除该消息\n\nSpringAMQP帮我们实现了消息确认。并允许我们通过配置文件设置ACK处理方式，有三种模式：\n\nnone：不处理。即消息投递给消费者后立刻ack，消息会立刻从MQ删除。非常不安全，不建议使用\nmanual：手动模式。需要自己在业务代码中调用api，发送ack或reject，存在业务入侵，但更灵活\nauto：自动模式。SpringAMQP利用AOP对我们的消息处理逻辑做了环绕增强，当业务正常执行时则自动返回ack.  当业务出现异常时，根据异常判断返回不同结果：\n如果是业务异常，会自动返回nack；\n如果是消息处理或校验异常，自动返回reject;\n\n\n\n实现通过下面的配置可以修改SpringAMQP的ACK处理方式：\nspring:  rabbitmq:    listener:      simple:        acknowledge-mode: none # 不做处理\n\n修改consumer服务的SpringRabbitListener类中的方法，模拟一个消息处理的异常：\n@RabbitListener(queues = &quot;simple.queue&quot;)public void listenSimpleQueueMessage(String msg) throws InterruptedException &#123;      log.info(&quot;消费者接收到消息：&quot; + msg );      throw new MessageConversionException(&quot;故意的&quot;);&#125;\n\n测试可以发现：当消息处理发生异常时，消息依然被RabbitMQ删除了。\n我们再次把确认机制修改为auto：\nspring:  rabbitmq:    listener:      simple:        acknowledge-mode: auto # 自动ack\n\n在异常位置打断点，再次发送消息，程序卡在断点时，可以发现此时消息状态为unacked（未确定状态）：\n\n\n\n消费者处理消息后可以向MQ发 送ack回执，MQ收到ack回执后才会删除该消息。\n\n但是当消费者出现异常后，消息会不断requeue（重入队）到队列，再重新发送给消费者。如果消费者再次执行依然出错，消息会再次requeue到队列，再次投递，直到消息处理成功为止。\n极端情况就是消费者一直无法执行成功，那么消息requeue就会无限循环，导致mq的消息处理飙升，带来不必要的压力，我们刚刚在验证auto模式时，就发现了一条消息也可以达到惊人的数据流量。\n为了应对上述情况Spring又提供了消费者失败重试机制：在消费者出现异常时利用本地重试，而不是无限制的requeue到mq队列。\n消费失败重试修改consumer服务的application.yml文件，添加内容：\nspring:  rabbitmq:    listener:      simple:        retry:          enabled: true # 开启消费者失败重试          initial-interval: 1000ms # 初识的失败等待时长为1秒          multiplier: 1 # 失败的等待时长倍数，下次等待时长 = multiplier * last-interval          max-attempts: 3 # 最大重试次数          stateless: true # true无状态；false有状态。如果业务中包含事务，这里改为false\n\n重启consumer服务，重复之前的测试。可以发现：\n\n消费者在失败后消息没有重新回到MQ无限重新投递，而是在本地重试了3次\n本地重试3次以后，抛出了AmqpRejectAndDontRequeueException异常。查看RabbitMQ控制台，发现消息被删除了，说明最后SpringAMQP返回的是reject\n\n结论：\n\n开启本地重试时，消息处理过程中抛出异常，不会requeue到队列，而是在消费者本地重试\n重试达到最大次数后，Spring会返回reject，消息会被丢弃\n\n\n异常交换机&#x2F;队列本地测试达到最大重试次数后，消息会被丢弃。这在某些对于消息可靠性要求较高的业务场景下，显然不太合适了。因此Spring允许我们自定义重试次数耗尽后的消息处理策略，这个策略是由MessageRecovery接口来定义的，它有3个不同实现：\n\nRejectAndDontRequeueRecoverer：重试耗尽后，直接reject，丢弃消息。默认就是这种方式 \nImmediateRequeueMessageRecoverer：重试耗尽后，返回nack，消息重新入队 \nRepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机\n\n比较优雅的一种处理方案是RepublishMessageRecoverer，失败后将消息投递到一个指定的，专门存放异常消息的队列，后续由人工集中处理。\n\n\n\n在consumer服务中定义处理失败消息的交换机和队列\n\n@Beanpublic DirectExchange errorMessageExchange()&#123;    return new DirectExchange(&quot;error.direct&quot;);&#125;@Beanpublic Queue errorQueue()&#123;    return new Queue(&quot;error.queue&quot;, true);&#125;@Beanpublic Binding errorBinding(Queue errorQueue, DirectExchange errorMessageExchange)&#123;    return BindingBuilder.bind(errorQueue).to(errorMessageExchange).with(&quot;error&quot;);&#125;\n\n\n定义一个RepublishMessageRecoverer，关联队列和交换机\n\n@Beanpublic MessageRecoverer republishMessageRecoverer(RabbitTemplate rabbitTemplate)&#123;    return new RepublishMessageRecoverer(rabbitTemplate, &quot;error.direct&quot;, &quot;error&quot;);&#125;\n\n\n运行\n\n最后可以看见，在接收三次消息之后，它将消息发给了error.queue里面还附带了全部的异常信息\n\n\n\n\n总结为了保证消费者的可靠性，首先我们通过消费者确认机制，消费者返回ack时，mq将消息删除；\n返回nack时，表示消息处理异常，为例减少服务器负担，我们在此基础上增加消费者重试机制，并声明异常交换机和异常处理器，将这些消息投递并路由到其中，最后由人工统一处理。\n\n如何保证消息的可靠性？消息从生产者一直到消费者被其消费的过程中，每个阶段都有可能导致消息的丢失，从而使业务异常，我们要从三个方面保证消息的可靠性：\n\n要保证生产者的可靠性：首先要通过confirm机制和return回调机制来实现生产者确认功能；confirm机制会在消息成功投递到交换机之后，返回ack，反之nack，来使消息投递到交换机的过程可感知化；但是消息成功投递到交换机中并不代表可以成功路由到队列上，所以还需要return回调机制来保证消息可以顺利路由到队列中；同时我们还可以定义confirmcallback和returncallback来设置不同情况下，我们应该做什么。\n要使消息持久化：消息持久化需要交换机，队列和消息本身都经过持久化，这样才可以把消息存储在磁盘上，即使mq挂掉，消息也不会消失。\n要保证消费者的可靠性：首先和生产者一样，我们使用消费者确认机制，在消息被成功消费之后返回ack，来通知mq这条消息可以删除，反之返回nack；但是为了业务的完整性，我们又使用消费者重试机制，保证消息可以被正常消费，同时又不过多占用服务器性能；在重试过后，我们可以设置一对异常处理交换器和队列，专门将重试过后但又没有被消费的消息统一管理起来，最后通过人工处理。\n\n至此，可以基本保证消息的可靠性。\n","categories":["MQ"],"tags":["MQ"]},{"title":"MySQL基础篇","url":"/2025/07/31/MySQL%E5%9F%BA%E7%A1%80%E7%AF%87/","content":""},{"title":"死信以及延迟消息","url":"/2025/07/28/%E6%AD%BB%E4%BF%A1%E4%BB%A5%E5%8F%8A%E5%BB%B6%E8%BF%9F%E6%B6%88%E6%81%AF/","content":"什么是死信死信，顾名思义就是无法被消费的消息. 当一个队列中的消息满足下 列情况之一时，可以成为死信（dead letter):\n当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）：\n\n消费者不要了（消费者使用basic.reject或 basic.nack声明消费失败，并且消息的requeue参数设置为false）。\n消息过期了（超过了队列或消息本身设置的过期时间）。\n投递的队列消息满了，最早的消息可能成为死信。\n\n如果一个队列中的消息已经成为死信，并且这个队列通过dead-letter-exchange属性指定了一个交换机，那么队列中的死信就会投递到这个交换机中，而这个交换机就称为死信交换机（Dead Letter Exchange ——DLX）。\n死信队列（Dead Letter Queue，简称 DLQ）是用于接收无法被正常消费的消息的一种特殊队列机制，RabbitMQ 提供死信交换机（DLX）来处理这些消息。\n他们之间的关系是这样的：死信 ➝ DLX ➝ routingKey ➝ 死信队列（DLQ）\n那他们有什么作用呢？\n\n收集那些因处理失败而被拒绝的消息\n收集那些因队列满了而被拒绝的消息\n1，2点其实就是对消费者异常处理的兜底手段\n\n\n收集因TTL（有效期）到期的消息（这个作用就可以让我们实现延迟消息的功能）\n\n\n延迟消息试想电影院购票、高铁购票，下单后就会锁定座位资源，其他人无法重复购买。\n但是这样就存在一个问题，假如用户下单后一直不付款，就会一直占有库存资源，导致其他客户无法正常交易，最终导致商户利益受损！\n因此，电商中通常的做法就是：对于超过一定时间未支付的订单，应该立刻取消订单并释放占用的库存。\n例如，订单支付超时时间为30分钟，则我们应该在用户下单后的第30分钟检查订单支付状态，如果发现未支付，应该立刻取消订单，释放库存。\n但问题来了：如何才能准确的实现在下单后第30分钟去检查支付状态呢？\n像这种在一段时间以后才执行的任务，我们称之为延迟任务，而要实现延迟任务，最简单的方案就是利用MQ的延迟消息了。\n在RabbitMQ中实现延迟消息也有两种方案：\n\n死信交换机+TTL（可以是队列的TTL也可以是消息本身的TTL）\n延迟消息插件\n\nDLX + TTL我们先缕清一下思路\n\n\n\n首先，生产者发送一个具有TTL（expiration属性决定）的消息给一个simple.direct，它会将消息发送给由dead-letter-exchange属性指定了死信交换机的simple.queue。\n这时消息因为没有消费者，所有会一直存在于simple.queue，一旦到达TTL所限制的时间，它就会成为死信，从而进入死信交换机dlx.direct。\n最后死信交换机将消息给dlx.queue，然后由对应的消费者处理延迟消息。\n\n\n我们编写好生产者和消费者的代码，验证一下\n\n生产者\n\n@Testvoid testSendTTLMessage2Queue()&#123;    Message msg = MessageBuilder            .withBody(&quot;101&quot;.getBytes(StandardCharsets.UTF_8))            .setExpiration(&quot;10000&quot;)            .build();    rabbitTemplate.convertAndSend(&quot;simple.direct&quot;,&quot;&quot;,msg);    log.info(&quot;我已发送消息&quot;);&#125;\n\n\n消费者\n\n@RabbitListener(queues = &quot;dlx.queue&quot;)public void listenerDlxQueue(String msg)&#123;    log.info(&quot;延时消费者接收到消息：&quot; + msg );&#125;\n\n\n运行\n\n\n\n可以看到，两次操作的时间正好是我们设置的10s，延迟消息实现成功。\n\n其实还可以设置队列的TTL来控制消息的过期时间，与上雷同，我就不演示了。\n\nDelayExchange插件基于死信队列虽然可以实现延迟消息，但是太麻烦了。因此RabbitMQ社区提供了一个延迟消息插件来实现相同的效果。官方文档说明：Scheduling Messages with RabbitMQ | RabbitMQ - Blog\n安装因为我们是基于Docker安装，所以需要先查看RabbitMQ的插件目录对应的数据卷。\ndocker volume inspect mq-plugins\n\n结果如下：\n[    &#123;        &quot;CreatedAt&quot;: &quot;2024-06-19T09:22:59+08:00&quot;,        &quot;Driver&quot;: &quot;local&quot;,        &quot;Labels&quot;: null,        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/mq-plugins/_data&quot;,        &quot;Name&quot;: &quot;mq-plugins&quot;,        &quot;Options&quot;: null,        &quot;Scope&quot;: &quot;local&quot;    &#125;]\n\n插件目录被挂载到了/var/lib/docker/volumes/mq-plugins/_data这个目录，我们上传插件到该目录下。\n接下来执行命令，安装插件：\ndocker exec -it mq rabbitmq-plugins enable rabbitmq_delayed_message_exchange\n\n\n\n即可\n\n声明延迟交换机\n注解的方式\n\n多增加一个属性delayed，将其属性值设置为true\n@RabbitListener(bindings = @QueueBinding(        value = @Queue(name = &quot;delay.queue&quot;, durable = &quot;true&quot;),        exchange = @Exchange(name = &quot;delay.direct&quot;, delayed = &quot;true&quot;),        key = &quot;delay&quot;))public void listenDelayMessage(String msg)&#123;    log.info(&quot;接收到delay.queue的延迟消息：&#123;&#125;&quot;, msg);&#125;\n\n\n基于@Bean的方式\n\n@Bean   public DirectExchange delayExchange()&#123;       return ExchangeBuilder               .directExchange(&quot;delay.direct&quot;) // 指定交换机类型和名称               .delayed() // 设置delay的属性为true               .durable(true) // 持久化               .build();   &#125;\n\n创建一个延迟交换机试试\n可以看到延迟转换器的参数和其他的是不同的。\n\n\n发送延迟消息发送消息时，必须通过x-delay属性设定延迟时间：\n@Testvoid testPublisherDelayMessage() &#123;    // 1.创建消息    String message = &quot;hello, delayed message&quot;;    // 2.发送消息，利用消息后置处理器添加消息头    rabbitTemplate.convertAndSend(&quot;delay.direct&quot;, &quot;delay&quot;, message, new MessagePostProcessor() &#123;        @Override        public Message postProcessMessage(Message message) throws AmqpException &#123;            // 添加延迟消息属性            message.getMessageProperties().setDelay(5000);            return message;        &#125;    &#125;);&#125;\n\n\n\n\n\n间隔5s，实现成功。\n注意：延迟消息插件内部会维护一个本地数据库表，同时使用Elang Timers功能实现计时。如果消息的延迟时间设置较长，可能会导致堆积的延迟消息非常多，会带来较大的CPU开销，同时延迟消息的时间会存在误差。因此，不建议设置延迟时间过长的延迟消息。\n\n总结死心队列&#x2F;交换机首先，我们要知道死信是什么，顾名思义就是死掉的消息，它不会再被消费者消费，它会满足以下三点之一：1.被消费者拒绝；2.消息的TTL过期；3.投递的队列满了，最开始进队列的消息就有可能成为死信；\n如果一个队列指定了deed-letter-exhange属性，它指定的这个交换机就成为死信交换机，死信交换机路由的队列就是死信队列。\n死信队列可以处理消费失败的消息，也可以处理过期消息。\n延迟队列延迟队列就是指消息到达mq之后不会立即被消费，它会等到设置的TTL过期之后，消费者才能拿到这个消息进行消费。\n比如订单的超时取消, 订单信息被放到mq中, 30分钟未支付订单就取消. 如果使用延时队列, 那监听mq的消费者从mq中直接拿到的就是，30分钟未支付订单的信息, 然后直接取消订单. 避免轮询数据库查找超时订单.\n实现延迟队列实现延迟队列我有两种策略\n\n发送消息时，给queue1中的消息设置TTL，然后通过交换机发送到queue1,queue1不绑定消费者，它的消息指向queue2，这样当TTL失效后，监听queue2的消费者就是接收的延迟消息；它的优点是RabbitMQ原生，缺点是不够精确，因为MQ会优先处理队头的消息。\n使用DelayExchange插件，通过给设置delay属性的方法声明延迟交换机，被其绑定的队列就是延迟队列，发送出来的带有TTL消息，被消费者消费时，就是延迟消息；它的优点是无需死信中转，精度更高，但是需要下载插件。\n\n","categories":["MQ"],"tags":["MQ"]},{"title":"消息的顺序性","url":"/2025/07/28/%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7/","content":"消息的顺序性错乱在业务中，可能会有对某个订单的增删改操作，比如有三条sql执行顺序是增加、修改、删除；消费者换了顺序给执行成删除、修改、增加，这样能行吗？\n肯定是不行的。\nRabbitMQ对于 RabbitMQ 来说，导致上面顺序错乱的原因通常是消费者是集群部署，不同的消费者消费到了同一订单的不同的消息，虽然消费者从MQ里面读取数据是有序的，但是可能因为每个消费者性能不同导致执行的顺序不同。\n如消费者 A 执行了增加，消费者 B 执行了修改，消费者 C 执行了删除，但是消费者 C 执行比消费者 B 快，消费者 B 又比消费者 A 快，就会导致执行到数据库的时候顺序错乱，本该顺序是增加、修改、删除，变成了删除、修改、增加。\n如图：\n\n\n\n\n如何保证消息的顺序性RabbitMQRabbitMQ 的问题是由于不同的消息都发送到了同一个queue中，多个消费者都消费同一个queue的消息。\n解决这个问题：\n\n我们可以给RabbitMQ创建多个queue，每个消费者固定消费一个queue的消息；\n生产者发送消息的时候，同一个订单号的消息发送到同一个queue中；\n由于同一个queue的消息是一定会保证有序的，那么同一个订单号的消息就只会被一个消费者顺序消费，从而保证了消息的顺序性。\n\n如图：\n\n\n\n会使队列变多, 造成吞吐量下降，但是这种可以在消费者内部采用多线程的方式去消费。\n\n总结RabbitMQ由于消费者的集群部署，同一个队列中的消息可能会被多个消费者消费，而每个消费者的性能不同，使用最终执行的顺序可能和我们预期的不同；\n我们可以采用创建多个queue的方式，一个业务的消息只发给某一个队列，这一个队列也只由一个消费者消费，由于队列内的消息是有序的，所以消费者消费的顺序也是有序的；\n只是这样会造成吞吐量的下降，但是我们可以在消费者内部采用多线程的方式消费。\n","categories":["MQ"],"tags":["MQ RabbitMQ"]},{"title":"消息积压的处理方法","url":"/2025/07/29/%E6%B6%88%E6%81%AF%E7%A7%AF%E5%8E%8B%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/","content":"消息积压的原因MQ 执行有三大阶段：\n\n消息生产阶段。\n消息存储阶段。\n消息消费阶段。\n\n很显然，消息堆积是出现在第三个消息消费阶段的。\n当生产者发送消息的速度超过了消费者处理消息的速度, 或者如果消费者因为某些原因持续阻塞，就会导致队列中的消息堆积，直到队列存储消息达到上限。最早接收到的消息，可能就会成为死信，会被丢弃，这就是消息积压问题\n解决办法如果仅仅是消息堆积，而且发现的及时\n增加更多消费者；\n提高消费速度提高单个消息者的处理能力：在消费者内开启线程池加快消息处理速度；\n使用限流手段，限制生产者生产消息的速度。\n\n如果是bug\n先修好消费者的bug，但是这个时间一般比较久，数据积压严重；\n我们提前建好10倍队列，再编写一个专门用于分发消息的消费者，停掉原来的消费者，用新的消费者将消息发给队列保存；\n最后消息处理完毕之后，寻找服务器空闲时间重新消费消息。\n\n","categories":["MQ"],"tags":["MQ RabbitMQ"]},{"title":"消息的幂等性","url":"/2025/07/28/%E6%B6%88%E6%81%AF%E7%9A%84%E5%B9%82%E7%AD%89%E6%80%A7/","content":"幂等性是什么幂等是一个数学概念，用函数表达式来描述是这样的：f(x) = f(f(x))，例如求绝对值函数。\n放在业务中，幂等就是i同一条消息被重复消费多次，业务结果只会被执行一次，不会引发副作用（如重复扣库存、重复发优惠券、重复扣款等），一句话就是指同一个业务，执行一次或多次对业务状态的影响是一致的。\n为什么要保证幂等性但数据的更新往往不是幂等的，如果重复执行可能造成不一样的后果。比如：\n\n取消订单，恢复库存的业务。如果多次恢复就会出现库存重复增加的情况\n退款业务。重复退款对商家而言会有经济损失。\n\n所以，我们要尽可能避免业务被重复执行。\n但是RabbitMQ 或任何 MQ 都不能 100% 保证只投递一次或只消费一次，可能因为：\n\n消息重复投递\n消息可能重新进入队列\n消息可能重复消费\n\n等等原因….\n所以在业务层保证幂等性显得尤为重要。\n如何保证幂等性数据库唯一约束如果从MQ拿到数据是要存到数据库，那么可以根据数据（创建一个UUID）创建唯一约束；这样的话，同样的数据从MQ发送过来之后，当插入数据库的时候，会报违反唯一约束的异常，不会插入成功的。\n\n每条消息携带唯一 ID（如 messageId、订单号）\n数据库建表并为 messageId 设置唯一约束：\n\nCREATE TABLE message_log (  message_id VARCHAR(64) PRIMARY KEY,  ...);\n\n\n消费者处理消息时，尝试插入 messageId：\n\n\n插入成功：说明是第一次处理 → 执行业务\n插入失败：违反唯一约束 → 表示已处理，跳过\n\n消息唯一id在生产者发送消息时，创建一个全局唯一id，在消费的时候，都先去redis里面查询是否有这个id，如果没有就执行业务插入id，如果有了，就跳过不执行业务。\n\nRedis 的 setIfAbsent 是原子操作，线程安全\n可设置过期时间，释放空间\n更适合高并发、轻量级幂等\n\n二者的共性及区别\n其实两种实现的基础都是在一个唯一id上，所以他们的共性就是在生成id上，这里我提供一种id的生成方式。\n\n共性我们翻阅源码，发现在消息转换器的底层中有这样一段：\n消息转换器的底层其实自动生成过一个id，我们只需要把CreateMessageIds的值设置为true即可使用。\n所以我们将之前的消息转换器改造一下：\n@Beanpublic MessageConverter messageConverter()&#123;    // 1.定义消息转换器    Jackson2JsonMessageConverter jjmc = new Jackson2JsonMessageConverter();    // 2.配置自动创建消息id，用于识别不同消息，也可以在业务中基于ID判断是否是重复消息    jjmc.setCreateMessageIds(true);    return jjmc;&#125;\n\n首先我们先不改变消息转换器，发送一条消息，然后改造之后再发送一条同样的消息。\n现在来测试一下:\n发现了改造之后的消息，携带了一个id!!\n\n\n这样唯一id的生成就搞定了。\n区别总结如下：\n\n\n\n比较项\n使用唯一 ID（Redis 去重）\n数据库唯一约束\n\n\n\n核心机制\n判断消息 ID 是否存在于 Redis\n利用数据库插入唯一值是否成功\n\n\n幂等性实现方式\n内存缓存去重\n主键&#x2F;唯一索引去重\n\n\n性能\n高性能，适合高并发场景\n有 IO 开销，适合稳定场景\n\n\n数据可追溯\n不可查历史\n可查历史处理记录\n\n\n存储压力\n小（可设置过期）\n大（需定期清理）\n\n\n事务一致性\nRedis 与业务逻辑分离\n可与业务逻辑在同一事务\n\n\n风险容错\nRedis 宕机可能丢幂等状态\n数据库更稳定\n\n\n总结实现消息的幂等性，我们可以有两种策略：\n\n使用数据库的唯一约束实现，从MQ中拿到消息是要将数据插入数据库，这个过程中可以把证明消息唯一的id设置为数据库的唯一约束；同样的数据插入数据库时，如果这个唯一约束已经存在，那就无法进行，如果时第一次，就可以顺利插入，从而实现了消息的幂等性。\n使用消息唯一id搭配redis，生产者每次发送消息时，生成一个全局唯一id；执行业务逻辑之前，我们都先查看redis中是否存在有这个唯一id；如果有，证明业务逻辑已经执行过，跳过不执行，反之执行，即可。\n\n","categories":["MQ"],"tags":["MQ"]}]